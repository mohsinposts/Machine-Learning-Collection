{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgJgdke/W0ubP6MIObS3oF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsinposts/Machine-Learning-Collection/blob/main/Iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0VSA-YoebSF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFtEMrLkfGvY",
        "outputId": "3b005228-6731-498c-b6d1-869d412582d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 4\n",
        "hidden_size = 250\n",
        "num_classes = 3\n",
        "batch_size = 8\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "kRWC87BSfKEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test data from iris dataset\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert data to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# create dataloader for data\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "TFpewclzwNYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "l-ZMK8PFewNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "T-9vPhyPhpC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    dataset_size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        batch_loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 1 == 0:\n",
        "            batch_loss, current = batch_loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"Progress:[{current:>5d}/{dataset_size:>5d}]  batch loss:{batch_loss:>8f}\")\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    average_loss = total_loss / num_batches\n",
        "    print(f\"Train Error: Avg loss: {average_loss:>8f}\")\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Get model pred\n",
        "            pred = model(X)\n",
        "\n",
        "            # Compute loss and\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= dataset_size\n",
        "\n",
        "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "IdkGUtCfhxtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
        "    train_losses.append(train_loss.detach().cpu())\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW1YXfish0jT",
        "outputId": "29c92252-38aa-40d4-fab5-61b23920bcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:1.074349\n",
            "Progress:[   16/  120]  batch loss:0.895170\n",
            "Progress:[   24/  120]  batch loss:1.475351\n",
            "Progress:[   32/  120]  batch loss:0.870727\n",
            "Progress:[   40/  120]  batch loss:0.818826\n",
            "Progress:[   48/  120]  batch loss:0.808519\n",
            "Progress:[   56/  120]  batch loss:0.863113\n",
            "Progress:[   64/  120]  batch loss:0.789540\n",
            "Progress:[   72/  120]  batch loss:0.681297\n",
            "Progress:[   80/  120]  batch loss:0.670470\n",
            "Progress:[   88/  120]  batch loss:0.692882\n",
            "Progress:[   96/  120]  batch loss:0.886348\n",
            "Progress:[  104/  120]  batch loss:0.598930\n",
            "Progress:[  112/  120]  batch loss:0.678328\n",
            "Progress:[  120/  120]  batch loss:0.602157\n",
            "Train Error: Avg loss: 0.827067\n",
            "Test Error: Accuracy: 96.7%, Avg loss: 0.564112\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.402301\n",
            "Progress:[   16/  120]  batch loss:0.431670\n",
            "Progress:[   24/  120]  batch loss:0.634996\n",
            "Progress:[   32/  120]  batch loss:0.441841\n",
            "Progress:[   40/  120]  batch loss:0.446490\n",
            "Progress:[   48/  120]  batch loss:0.588051\n",
            "Progress:[   56/  120]  batch loss:0.412551\n",
            "Progress:[   64/  120]  batch loss:0.538601\n",
            "Progress:[   72/  120]  batch loss:0.372792\n",
            "Progress:[   80/  120]  batch loss:0.405290\n",
            "Progress:[   88/  120]  batch loss:0.545292\n",
            "Progress:[   96/  120]  batch loss:0.658241\n",
            "Progress:[  104/  120]  batch loss:0.398015\n",
            "Progress:[  112/  120]  batch loss:0.528795\n",
            "Progress:[  120/  120]  batch loss:0.403448\n",
            "Train Error: Avg loss: 0.480558\n",
            "Test Error: Accuracy: 83.3%, Avg loss: 0.392075\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.245362\n",
            "Progress:[   16/  120]  batch loss:0.311202\n",
            "Progress:[   24/  120]  batch loss:0.489968\n",
            "Progress:[   32/  120]  batch loss:0.303468\n",
            "Progress:[   40/  120]  batch loss:0.303566\n",
            "Progress:[   48/  120]  batch loss:0.479140\n",
            "Progress:[   56/  120]  batch loss:0.283018\n",
            "Progress:[   64/  120]  batch loss:0.438453\n",
            "Progress:[   72/  120]  batch loss:0.332457\n",
            "Progress:[   80/  120]  batch loss:0.325194\n",
            "Progress:[   88/  120]  batch loss:0.449846\n",
            "Progress:[   96/  120]  batch loss:0.480137\n",
            "Progress:[  104/  120]  batch loss:0.304964\n",
            "Progress:[  112/  120]  batch loss:0.406580\n",
            "Progress:[  120/  120]  batch loss:0.282734\n",
            "Train Error: Avg loss: 0.362406\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.305823\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.177487\n",
            "Progress:[   16/  120]  batch loss:0.245250\n",
            "Progress:[   24/  120]  batch loss:0.388134\n",
            "Progress:[   32/  120]  batch loss:0.219439\n",
            "Progress:[   40/  120]  batch loss:0.216570\n",
            "Progress:[   48/  120]  batch loss:0.391192\n",
            "Progress:[   56/  120]  batch loss:0.203027\n",
            "Progress:[   64/  120]  batch loss:0.372971\n",
            "Progress:[   72/  120]  batch loss:0.298252\n",
            "Progress:[   80/  120]  batch loss:0.258696\n",
            "Progress:[   88/  120]  batch loss:0.384570\n",
            "Progress:[   96/  120]  batch loss:0.348690\n",
            "Progress:[  104/  120]  batch loss:0.213991\n",
            "Progress:[  112/  120]  batch loss:0.291599\n",
            "Progress:[  120/  120]  batch loss:0.176689\n",
            "Train Error: Avg loss: 0.279104\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.240023\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.120913\n",
            "Progress:[   16/  120]  batch loss:0.185533\n",
            "Progress:[   24/  120]  batch loss:0.299770\n",
            "Progress:[   32/  120]  batch loss:0.154108\n",
            "Progress:[   40/  120]  batch loss:0.145007\n",
            "Progress:[   48/  120]  batch loss:0.308652\n",
            "Progress:[   56/  120]  batch loss:0.138415\n",
            "Progress:[   64/  120]  batch loss:0.318443\n",
            "Progress:[   72/  120]  batch loss:0.275138\n",
            "Progress:[   80/  120]  batch loss:0.199333\n",
            "Progress:[   88/  120]  batch loss:0.343626\n",
            "Progress:[   96/  120]  batch loss:0.244383\n",
            "Progress:[  104/  120]  batch loss:0.132732\n",
            "Progress:[  112/  120]  batch loss:0.205754\n",
            "Progress:[  120/  120]  batch loss:0.101708\n",
            "Train Error: Avg loss: 0.211568\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.199666\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.081373\n",
            "Progress:[   16/  120]  batch loss:0.146603\n",
            "Progress:[   24/  120]  batch loss:0.245290\n",
            "Progress:[   32/  120]  batch loss:0.107152\n",
            "Progress:[   40/  120]  batch loss:0.096833\n",
            "Progress:[   48/  120]  batch loss:0.248643\n",
            "Progress:[   56/  120]  batch loss:0.096624\n",
            "Progress:[   64/  120]  batch loss:0.278969\n",
            "Progress:[   72/  120]  batch loss:0.265155\n",
            "Progress:[   80/  120]  batch loss:0.160631\n",
            "Progress:[   88/  120]  batch loss:0.348761\n",
            "Progress:[   96/  120]  batch loss:0.165382\n",
            "Progress:[  104/  120]  batch loss:0.086413\n",
            "Progress:[  112/  120]  batch loss:0.155572\n",
            "Progress:[  120/  120]  batch loss:0.063536\n",
            "Train Error: Avg loss: 0.169796\n",
            "Test Error: Accuracy: 86.7%, Avg loss: 0.192670\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.061837\n",
            "Progress:[   16/  120]  batch loss:0.141242\n",
            "Progress:[   24/  120]  batch loss:0.239980\n",
            "Progress:[   32/  120]  batch loss:0.075209\n",
            "Progress:[   40/  120]  batch loss:0.073911\n",
            "Progress:[   48/  120]  batch loss:0.208198\n",
            "Progress:[   56/  120]  batch loss:0.069875\n",
            "Progress:[   64/  120]  batch loss:0.250643\n",
            "Progress:[   72/  120]  batch loss:0.280153\n",
            "Progress:[   80/  120]  batch loss:0.141813\n",
            "Progress:[   88/  120]  batch loss:0.416642\n",
            "Progress:[   96/  120]  batch loss:0.102412\n",
            "Progress:[  104/  120]  batch loss:0.067288\n",
            "Progress:[  112/  120]  batch loss:0.119759\n",
            "Progress:[  120/  120]  batch loss:0.044961\n",
            "Train Error: Avg loss: 0.152928\n",
            "Test Error: Accuracy: 83.3%, Avg loss: 0.216169\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.056043\n",
            "Progress:[   16/  120]  batch loss:0.169449\n",
            "Progress:[   24/  120]  batch loss:0.293197\n",
            "Progress:[   32/  120]  batch loss:0.062491\n",
            "Progress:[   40/  120]  batch loss:0.076287\n",
            "Progress:[   48/  120]  batch loss:0.196702\n",
            "Progress:[   56/  120]  batch loss:0.056238\n",
            "Progress:[   64/  120]  batch loss:0.243446\n",
            "Progress:[   72/  120]  batch loss:0.283203\n",
            "Progress:[   80/  120]  batch loss:0.148723\n",
            "Progress:[   88/  120]  batch loss:0.589501\n",
            "Progress:[   96/  120]  batch loss:0.053512\n",
            "Progress:[  104/  120]  batch loss:0.082159\n",
            "Progress:[  112/  120]  batch loss:0.074241\n",
            "Progress:[  120/  120]  batch loss:0.030354\n",
            "Train Error: Avg loss: 0.161036\n",
            "Test Error: Accuracy: 83.3%, Avg loss: 0.219660\n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.045649\n",
            "Progress:[   16/  120]  batch loss:0.187653\n",
            "Progress:[   24/  120]  batch loss:0.368463\n",
            "Progress:[   32/  120]  batch loss:0.070035\n",
            "Progress:[   40/  120]  batch loss:0.112849\n",
            "Progress:[   48/  120]  batch loss:0.251205\n",
            "Progress:[   56/  120]  batch loss:0.069893\n",
            "Progress:[   64/  120]  batch loss:0.303756\n",
            "Progress:[   72/  120]  batch loss:0.199457\n",
            "Progress:[   80/  120]  batch loss:0.148204\n",
            "Progress:[   88/  120]  batch loss:0.824571\n",
            "Progress:[   96/  120]  batch loss:0.027316\n",
            "Progress:[  104/  120]  batch loss:0.194961\n",
            "Progress:[  112/  120]  batch loss:0.045885\n",
            "Progress:[  120/  120]  batch loss:0.022096\n",
            "Train Error: Avg loss: 0.191466\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.134325\n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.019374\n",
            "Progress:[   16/  120]  batch loss:0.101396\n",
            "Progress:[   24/  120]  batch loss:0.319194\n",
            "Progress:[   32/  120]  batch loss:0.070348\n",
            "Progress:[   40/  120]  batch loss:0.159964\n",
            "Progress:[   48/  120]  batch loss:0.380239\n",
            "Progress:[   56/  120]  batch loss:0.152090\n",
            "Progress:[   64/  120]  batch loss:0.526360\n",
            "Progress:[   72/  120]  batch loss:0.062470\n",
            "Progress:[   80/  120]  batch loss:0.092220\n",
            "Progress:[   88/  120]  batch loss:0.718757\n",
            "Progress:[   96/  120]  batch loss:0.021269\n",
            "Progress:[  104/  120]  batch loss:0.381169\n",
            "Progress:[  112/  120]  batch loss:0.096030\n",
            "Progress:[  120/  120]  batch loss:0.051992\n",
            "Train Error: Avg loss: 0.210191\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.092177\n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.013977\n",
            "Progress:[   16/  120]  batch loss:0.024929\n",
            "Progress:[   24/  120]  batch loss:0.156495\n",
            "Progress:[   32/  120]  batch loss:0.038444\n",
            "Progress:[   40/  120]  batch loss:0.101889\n",
            "Progress:[   48/  120]  batch loss:0.367814\n",
            "Progress:[   56/  120]  batch loss:0.202361\n",
            "Progress:[   64/  120]  batch loss:0.717174\n",
            "Progress:[   72/  120]  batch loss:0.032792\n",
            "Progress:[   80/  120]  batch loss:0.134530\n",
            "Progress:[   88/  120]  batch loss:0.261884\n",
            "Progress:[   96/  120]  batch loss:0.027018\n",
            "Progress:[  104/  120]  batch loss:0.266184\n",
            "Progress:[  112/  120]  batch loss:0.102373\n",
            "Progress:[  120/  120]  batch loss:0.092245\n",
            "Train Error: Avg loss: 0.169341\n",
            "Test Error: Accuracy: 86.7%, Avg loss: 0.155883\n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.027031\n",
            "Progress:[   16/  120]  batch loss:0.011496\n",
            "Progress:[   24/  120]  batch loss:0.125258\n",
            "Progress:[   32/  120]  batch loss:0.041169\n",
            "Progress:[   40/  120]  batch loss:0.028819\n",
            "Progress:[   48/  120]  batch loss:0.190266\n",
            "Progress:[   56/  120]  batch loss:0.108665\n",
            "Progress:[   64/  120]  batch loss:0.604723\n",
            "Progress:[   72/  120]  batch loss:0.031375\n",
            "Progress:[   80/  120]  batch loss:0.173219\n",
            "Progress:[   88/  120]  batch loss:0.145235\n",
            "Progress:[   96/  120]  batch loss:0.038939\n",
            "Progress:[  104/  120]  batch loss:0.132369\n",
            "Progress:[  112/  120]  batch loss:0.054242\n",
            "Progress:[  120/  120]  batch loss:0.061471\n",
            "Train Error: Avg loss: 0.118285\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.135611\n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.021506\n",
            "Progress:[   16/  120]  batch loss:0.010296\n",
            "Progress:[   24/  120]  batch loss:0.137032\n",
            "Progress:[   32/  120]  batch loss:0.051620\n",
            "Progress:[   40/  120]  batch loss:0.018692\n",
            "Progress:[   48/  120]  batch loss:0.146864\n",
            "Progress:[   56/  120]  batch loss:0.067189\n",
            "Progress:[   64/  120]  batch loss:0.510546\n",
            "Progress:[   72/  120]  batch loss:0.028952\n",
            "Progress:[   80/  120]  batch loss:0.145673\n",
            "Progress:[   88/  120]  batch loss:0.158582\n",
            "Progress:[   96/  120]  batch loss:0.034980\n",
            "Progress:[  104/  120]  batch loss:0.110569\n",
            "Progress:[  112/  120]  batch loss:0.040717\n",
            "Progress:[  120/  120]  batch loss:0.043125\n",
            "Train Error: Avg loss: 0.101756\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.107395\n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.014320\n",
            "Progress:[   16/  120]  batch loss:0.008670\n",
            "Progress:[   24/  120]  batch loss:0.110854\n",
            "Progress:[   32/  120]  batch loss:0.038426\n",
            "Progress:[   40/  120]  batch loss:0.019069\n",
            "Progress:[   48/  120]  batch loss:0.148308\n",
            "Progress:[   56/  120]  batch loss:0.070364\n",
            "Progress:[   64/  120]  batch loss:0.518296\n",
            "Progress:[   72/  120]  batch loss:0.026684\n",
            "Progress:[   80/  120]  batch loss:0.129389\n",
            "Progress:[   88/  120]  batch loss:0.189548\n",
            "Progress:[   96/  120]  batch loss:0.026304\n",
            "Progress:[  104/  120]  batch loss:0.129587\n",
            "Progress:[  112/  120]  batch loss:0.041915\n",
            "Progress:[  120/  120]  batch loss:0.044221\n",
            "Train Error: Avg loss: 0.101064\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.106379\n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.013160\n",
            "Progress:[   16/  120]  batch loss:0.007564\n",
            "Progress:[   24/  120]  batch loss:0.103541\n",
            "Progress:[   32/  120]  batch loss:0.033143\n",
            "Progress:[   40/  120]  batch loss:0.018966\n",
            "Progress:[   48/  120]  batch loss:0.151639\n",
            "Progress:[   56/  120]  batch loss:0.078469\n",
            "Progress:[   64/  120]  batch loss:0.545284\n",
            "Progress:[   72/  120]  batch loss:0.024060\n",
            "Progress:[   80/  120]  batch loss:0.131499\n",
            "Progress:[   88/  120]  batch loss:0.190118\n",
            "Progress:[   96/  120]  batch loss:0.022553\n",
            "Progress:[  104/  120]  batch loss:0.137663\n",
            "Progress:[  112/  120]  batch loss:0.042849\n",
            "Progress:[  120/  120]  batch loss:0.046887\n",
            "Train Error: Avg loss: 0.103160\n",
            "Test Error: Accuracy: 96.7%, Avg loss: 0.110898\n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.013097\n",
            "Progress:[   16/  120]  batch loss:0.006648\n",
            "Progress:[   24/  120]  batch loss:0.103356\n",
            "Progress:[   32/  120]  batch loss:0.031954\n",
            "Progress:[   40/  120]  batch loss:0.016919\n",
            "Progress:[   48/  120]  batch loss:0.146872\n",
            "Progress:[   56/  120]  batch loss:0.079220\n",
            "Progress:[   64/  120]  batch loss:0.556638\n",
            "Progress:[   72/  120]  batch loss:0.022136\n",
            "Progress:[   80/  120]  batch loss:0.137952\n",
            "Progress:[   88/  120]  batch loss:0.179060\n",
            "Progress:[   96/  120]  batch loss:0.020714\n",
            "Progress:[  104/  120]  batch loss:0.134588\n",
            "Progress:[  112/  120]  batch loss:0.041350\n",
            "Progress:[  120/  120]  batch loss:0.047272\n",
            "Train Error: Avg loss: 0.102518\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.113928\n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.012681\n",
            "Progress:[   16/  120]  batch loss:0.005942\n",
            "Progress:[   24/  120]  batch loss:0.104073\n",
            "Progress:[   32/  120]  batch loss:0.031748\n",
            "Progress:[   40/  120]  batch loss:0.014746\n",
            "Progress:[   48/  120]  batch loss:0.139666\n",
            "Progress:[   56/  120]  batch loss:0.076415\n",
            "Progress:[   64/  120]  batch loss:0.560257\n",
            "Progress:[   72/  120]  batch loss:0.020433\n",
            "Progress:[   80/  120]  batch loss:0.140836\n",
            "Progress:[   88/  120]  batch loss:0.171411\n",
            "Progress:[   96/  120]  batch loss:0.019335\n",
            "Progress:[  104/  120]  batch loss:0.129260\n",
            "Progress:[  112/  120]  batch loss:0.038738\n",
            "Progress:[  120/  120]  batch loss:0.046025\n",
            "Train Error: Avg loss: 0.100771\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.114023\n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.012038\n",
            "Progress:[   16/  120]  batch loss:0.005372\n",
            "Progress:[   24/  120]  batch loss:0.102895\n",
            "Progress:[   32/  120]  batch loss:0.031163\n",
            "Progress:[   40/  120]  batch loss:0.013232\n",
            "Progress:[   48/  120]  batch loss:0.133662\n",
            "Progress:[   56/  120]  batch loss:0.074627\n",
            "Progress:[   64/  120]  batch loss:0.562793\n",
            "Progress:[   72/  120]  batch loss:0.018992\n",
            "Progress:[   80/  120]  batch loss:0.141662\n",
            "Progress:[   88/  120]  batch loss:0.167755\n",
            "Progress:[   96/  120]  batch loss:0.017812\n",
            "Progress:[  104/  120]  batch loss:0.125912\n",
            "Progress:[  112/  120]  batch loss:0.036667\n",
            "Progress:[  120/  120]  batch loss:0.045057\n",
            "Train Error: Avg loss: 0.099309\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.114534\n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.011502\n",
            "Progress:[   16/  120]  batch loss:0.004902\n",
            "Progress:[   24/  120]  batch loss:0.102450\n",
            "Progress:[   32/  120]  batch loss:0.030493\n",
            "Progress:[   40/  120]  batch loss:0.011920\n",
            "Progress:[   48/  120]  batch loss:0.128588\n",
            "Progress:[   56/  120]  batch loss:0.073133\n",
            "Progress:[   64/  120]  batch loss:0.567486\n",
            "Progress:[   72/  120]  batch loss:0.017590\n",
            "Progress:[   80/  120]  batch loss:0.145158\n",
            "Progress:[   88/  120]  batch loss:0.162487\n",
            "Progress:[   96/  120]  batch loss:0.016508\n",
            "Progress:[  104/  120]  batch loss:0.122881\n",
            "Progress:[  112/  120]  batch loss:0.035200\n",
            "Progress:[  120/  120]  batch loss:0.044640\n",
            "Train Error: Avg loss: 0.098329\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.115965\n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.010762\n",
            "Progress:[   16/  120]  batch loss:0.004482\n",
            "Progress:[   24/  120]  batch loss:0.102296\n",
            "Progress:[   32/  120]  batch loss:0.029519\n",
            "Progress:[   40/  120]  batch loss:0.010840\n",
            "Progress:[   48/  120]  batch loss:0.123920\n",
            "Progress:[   56/  120]  batch loss:0.072065\n",
            "Progress:[   64/  120]  batch loss:0.570560\n",
            "Progress:[   72/  120]  batch loss:0.016534\n",
            "Progress:[   80/  120]  batch loss:0.147752\n",
            "Progress:[   88/  120]  batch loss:0.157550\n",
            "Progress:[   96/  120]  batch loss:0.015366\n",
            "Progress:[  104/  120]  batch loss:0.120265\n",
            "Progress:[  112/  120]  batch loss:0.034238\n",
            "Progress:[  120/  120]  batch loss:0.044176\n",
            "Train Error: Avg loss: 0.097355\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.117161\n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.010525\n",
            "Progress:[   16/  120]  batch loss:0.004166\n",
            "Progress:[   24/  120]  batch loss:0.102846\n",
            "Progress:[   32/  120]  batch loss:0.029435\n",
            "Progress:[   40/  120]  batch loss:0.009770\n",
            "Progress:[   48/  120]  batch loss:0.119407\n",
            "Progress:[   56/  120]  batch loss:0.070141\n",
            "Progress:[   64/  120]  batch loss:0.574463\n",
            "Progress:[   72/  120]  batch loss:0.015496\n",
            "Progress:[   80/  120]  batch loss:0.149803\n",
            "Progress:[   88/  120]  batch loss:0.152027\n",
            "Progress:[   96/  120]  batch loss:0.014534\n",
            "Progress:[  104/  120]  batch loss:0.116544\n",
            "Progress:[  112/  120]  batch loss:0.032757\n",
            "Progress:[  120/  120]  batch loss:0.043377\n",
            "Train Error: Avg loss: 0.096353\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.117804\n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.010292\n",
            "Progress:[   16/  120]  batch loss:0.003894\n",
            "Progress:[   24/  120]  batch loss:0.104376\n",
            "Progress:[   32/  120]  batch loss:0.029709\n",
            "Progress:[   40/  120]  batch loss:0.008834\n",
            "Progress:[   48/  120]  batch loss:0.114978\n",
            "Progress:[   56/  120]  batch loss:0.068294\n",
            "Progress:[   64/  120]  batch loss:0.576736\n",
            "Progress:[   72/  120]  batch loss:0.014561\n",
            "Progress:[   80/  120]  batch loss:0.152916\n",
            "Progress:[   88/  120]  batch loss:0.146470\n",
            "Progress:[   96/  120]  batch loss:0.013758\n",
            "Progress:[  104/  120]  batch loss:0.112741\n",
            "Progress:[  112/  120]  batch loss:0.031504\n",
            "Progress:[  120/  120]  batch loss:0.042998\n",
            "Train Error: Avg loss: 0.095471\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.119831\n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.009823\n",
            "Progress:[   16/  120]  batch loss:0.003649\n",
            "Progress:[   24/  120]  batch loss:0.105528\n",
            "Progress:[   32/  120]  batch loss:0.029371\n",
            "Progress:[   40/  120]  batch loss:0.008053\n",
            "Progress:[   48/  120]  batch loss:0.110969\n",
            "Progress:[   56/  120]  batch loss:0.066730\n",
            "Progress:[   64/  120]  batch loss:0.578768\n",
            "Progress:[   72/  120]  batch loss:0.013733\n",
            "Progress:[   80/  120]  batch loss:0.156830\n",
            "Progress:[   88/  120]  batch loss:0.141365\n",
            "Progress:[   96/  120]  batch loss:0.013026\n",
            "Progress:[  104/  120]  batch loss:0.109722\n",
            "Progress:[  112/  120]  batch loss:0.030337\n",
            "Progress:[  120/  120]  batch loss:0.042276\n",
            "Train Error: Avg loss: 0.094679\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.120118\n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.009490\n",
            "Progress:[   16/  120]  batch loss:0.003452\n",
            "Progress:[   24/  120]  batch loss:0.106060\n",
            "Progress:[   32/  120]  batch loss:0.029019\n",
            "Progress:[   40/  120]  batch loss:0.007436\n",
            "Progress:[   48/  120]  batch loss:0.107710\n",
            "Progress:[   56/  120]  batch loss:0.065438\n",
            "Progress:[   64/  120]  batch loss:0.579609\n",
            "Progress:[   72/  120]  batch loss:0.013149\n",
            "Progress:[   80/  120]  batch loss:0.158827\n",
            "Progress:[   88/  120]  batch loss:0.136972\n",
            "Progress:[   96/  120]  batch loss:0.012296\n",
            "Progress:[  104/  120]  batch loss:0.108999\n",
            "Progress:[  112/  120]  batch loss:0.029963\n",
            "Progress:[  120/  120]  batch loss:0.042424\n",
            "Train Error: Avg loss: 0.094056\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.123237\n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.009414\n",
            "Progress:[   16/  120]  batch loss:0.003287\n",
            "Progress:[   24/  120]  batch loss:0.108782\n",
            "Progress:[   32/  120]  batch loss:0.030056\n",
            "Progress:[   40/  120]  batch loss:0.006692\n",
            "Progress:[   48/  120]  batch loss:0.104163\n",
            "Progress:[   56/  120]  batch loss:0.062779\n",
            "Progress:[   64/  120]  batch loss:0.586031\n",
            "Progress:[   72/  120]  batch loss:0.012130\n",
            "Progress:[   80/  120]  batch loss:0.161675\n",
            "Progress:[   88/  120]  batch loss:0.130982\n",
            "Progress:[   96/  120]  batch loss:0.011998\n",
            "Progress:[  104/  120]  batch loss:0.102333\n",
            "Progress:[  112/  120]  batch loss:0.027588\n",
            "Progress:[  120/  120]  batch loss:0.040394\n",
            "Train Error: Avg loss: 0.093220\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.120924\n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008970\n",
            "Progress:[   16/  120]  batch loss:0.003106\n",
            "Progress:[   24/  120]  batch loss:0.108353\n",
            "Progress:[   32/  120]  batch loss:0.030340\n",
            "Progress:[   40/  120]  batch loss:0.006208\n",
            "Progress:[   48/  120]  batch loss:0.101201\n",
            "Progress:[   56/  120]  batch loss:0.060463\n",
            "Progress:[   64/  120]  batch loss:0.581147\n",
            "Progress:[   72/  120]  batch loss:0.011804\n",
            "Progress:[   80/  120]  batch loss:0.161744\n",
            "Progress:[   88/  120]  batch loss:0.128496\n",
            "Progress:[   96/  120]  batch loss:0.011461\n",
            "Progress:[  104/  120]  batch loss:0.100246\n",
            "Progress:[  112/  120]  batch loss:0.027433\n",
            "Progress:[  120/  120]  batch loss:0.040558\n",
            "Train Error: Avg loss: 0.092102\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.122765\n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008907\n",
            "Progress:[   16/  120]  batch loss:0.002996\n",
            "Progress:[   24/  120]  batch loss:0.111080\n",
            "Progress:[   32/  120]  batch loss:0.030397\n",
            "Progress:[   40/  120]  batch loss:0.005722\n",
            "Progress:[   48/  120]  batch loss:0.098613\n",
            "Progress:[   56/  120]  batch loss:0.059087\n",
            "Progress:[   64/  120]  batch loss:0.584991\n",
            "Progress:[   72/  120]  batch loss:0.011070\n",
            "Progress:[   80/  120]  batch loss:0.166623\n",
            "Progress:[   88/  120]  batch loss:0.122936\n",
            "Progress:[   96/  120]  batch loss:0.011052\n",
            "Progress:[  104/  120]  batch loss:0.095964\n",
            "Progress:[  112/  120]  batch loss:0.025900\n",
            "Progress:[  120/  120]  batch loss:0.039685\n",
            "Train Error: Avg loss: 0.091668\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.123573\n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008388\n",
            "Progress:[   16/  120]  batch loss:0.002855\n",
            "Progress:[   24/  120]  batch loss:0.112613\n",
            "Progress:[   32/  120]  batch loss:0.030268\n",
            "Progress:[   40/  120]  batch loss:0.005371\n",
            "Progress:[   48/  120]  batch loss:0.096525\n",
            "Progress:[   56/  120]  batch loss:0.058058\n",
            "Progress:[   64/  120]  batch loss:0.583527\n",
            "Progress:[   72/  120]  batch loss:0.010712\n",
            "Progress:[   80/  120]  batch loss:0.168948\n",
            "Progress:[   88/  120]  batch loss:0.119525\n",
            "Progress:[   96/  120]  batch loss:0.010668\n",
            "Progress:[  104/  120]  batch loss:0.093816\n",
            "Progress:[  112/  120]  batch loss:0.025581\n",
            "Progress:[  120/  120]  batch loss:0.039576\n",
            "Train Error: Avg loss: 0.091095\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.124960\n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008330\n",
            "Progress:[   16/  120]  batch loss:0.002791\n",
            "Progress:[   24/  120]  batch loss:0.115098\n",
            "Progress:[   32/  120]  batch loss:0.030757\n",
            "Progress:[   40/  120]  batch loss:0.005014\n",
            "Progress:[   48/  120]  batch loss:0.094546\n",
            "Progress:[   56/  120]  batch loss:0.056639\n",
            "Progress:[   64/  120]  batch loss:0.584919\n",
            "Progress:[   72/  120]  batch loss:0.010257\n",
            "Progress:[   80/  120]  batch loss:0.172163\n",
            "Progress:[   88/  120]  batch loss:0.114533\n",
            "Progress:[   96/  120]  batch loss:0.010401\n",
            "Progress:[  104/  120]  batch loss:0.089893\n",
            "Progress:[  112/  120]  batch loss:0.024777\n",
            "Progress:[  120/  120]  batch loss:0.039079\n",
            "Train Error: Avg loss: 0.090613\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.125031\n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008246\n",
            "Progress:[   16/  120]  batch loss:0.002715\n",
            "Progress:[   24/  120]  batch loss:0.116658\n",
            "Progress:[   32/  120]  batch loss:0.031609\n",
            "Progress:[   40/  120]  batch loss:0.004693\n",
            "Progress:[   48/  120]  batch loss:0.092783\n",
            "Progress:[   56/  120]  batch loss:0.054439\n",
            "Progress:[   64/  120]  batch loss:0.583389\n",
            "Progress:[   72/  120]  batch loss:0.009892\n",
            "Progress:[   80/  120]  batch loss:0.171217\n",
            "Progress:[   88/  120]  batch loss:0.111867\n",
            "Progress:[   96/  120]  batch loss:0.010197\n",
            "Progress:[  104/  120]  batch loss:0.087337\n",
            "Progress:[  112/  120]  batch loss:0.023725\n",
            "Progress:[  120/  120]  batch loss:0.037633\n",
            "Train Error: Avg loss: 0.089760\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.122891\n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.008032\n",
            "Progress:[   16/  120]  batch loss:0.002594\n",
            "Progress:[   24/  120]  batch loss:0.115375\n",
            "Progress:[   32/  120]  batch loss:0.031690\n",
            "Progress:[   40/  120]  batch loss:0.004423\n",
            "Progress:[   48/  120]  batch loss:0.091084\n",
            "Progress:[   56/  120]  batch loss:0.052783\n",
            "Progress:[   64/  120]  batch loss:0.582727\n",
            "Progress:[   72/  120]  batch loss:0.009559\n",
            "Progress:[   80/  120]  batch loss:0.171382\n",
            "Progress:[   88/  120]  batch loss:0.110026\n",
            "Progress:[   96/  120]  batch loss:0.009952\n",
            "Progress:[  104/  120]  batch loss:0.084902\n",
            "Progress:[  112/  120]  batch loss:0.023258\n",
            "Progress:[  120/  120]  batch loss:0.037276\n",
            "Train Error: Avg loss: 0.089004\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.121358\n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.007920\n",
            "Progress:[   16/  120]  batch loss:0.002532\n",
            "Progress:[   24/  120]  batch loss:0.115331\n",
            "Progress:[   32/  120]  batch loss:0.031528\n",
            "Progress:[   40/  120]  batch loss:0.004220\n",
            "Progress:[   48/  120]  batch loss:0.089853\n",
            "Progress:[   56/  120]  batch loss:0.051658\n",
            "Progress:[   64/  120]  batch loss:0.579402\n",
            "Progress:[   72/  120]  batch loss:0.009278\n",
            "Progress:[   80/  120]  batch loss:0.172666\n",
            "Progress:[   88/  120]  batch loss:0.107150\n",
            "Progress:[   96/  120]  batch loss:0.009786\n",
            "Progress:[  104/  120]  batch loss:0.080914\n",
            "Progress:[  112/  120]  batch loss:0.022852\n",
            "Progress:[  120/  120]  batch loss:0.036689\n",
            "Train Error: Avg loss: 0.088119\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.121910\n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.007679\n",
            "Progress:[   16/  120]  batch loss:0.002492\n",
            "Progress:[   24/  120]  batch loss:0.117331\n",
            "Progress:[   32/  120]  batch loss:0.031642\n",
            "Progress:[   40/  120]  batch loss:0.004033\n",
            "Progress:[   48/  120]  batch loss:0.088929\n",
            "Progress:[   56/  120]  batch loss:0.050661\n",
            "Progress:[   64/  120]  batch loss:0.577013\n",
            "Progress:[   72/  120]  batch loss:0.009027\n",
            "Progress:[   80/  120]  batch loss:0.173528\n",
            "Progress:[   88/  120]  batch loss:0.104610\n",
            "Progress:[   96/  120]  batch loss:0.009408\n",
            "Progress:[  104/  120]  batch loss:0.080941\n",
            "Progress:[  112/  120]  batch loss:0.022120\n",
            "Progress:[  120/  120]  batch loss:0.036561\n",
            "Train Error: Avg loss: 0.087732\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.123378\n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.007402\n",
            "Progress:[   16/  120]  batch loss:0.002379\n",
            "Progress:[   24/  120]  batch loss:0.119348\n",
            "Progress:[   32/  120]  batch loss:0.031515\n",
            "Progress:[   40/  120]  batch loss:0.003769\n",
            "Progress:[   48/  120]  batch loss:0.087788\n",
            "Progress:[   56/  120]  batch loss:0.049199\n",
            "Progress:[   64/  120]  batch loss:0.581971\n",
            "Progress:[   72/  120]  batch loss:0.008487\n",
            "Progress:[   80/  120]  batch loss:0.177721\n",
            "Progress:[   88/  120]  batch loss:0.101769\n",
            "Progress:[   96/  120]  batch loss:0.008974\n",
            "Progress:[  104/  120]  batch loss:0.078331\n",
            "Progress:[  112/  120]  batch loss:0.020880\n",
            "Progress:[  120/  120]  batch loss:0.035431\n",
            "Train Error: Avg loss: 0.087664\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.125149\n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.007137\n",
            "Progress:[   16/  120]  batch loss:0.002281\n",
            "Progress:[   24/  120]  batch loss:0.121258\n",
            "Progress:[   32/  120]  batch loss:0.031570\n",
            "Progress:[   40/  120]  batch loss:0.003563\n",
            "Progress:[   48/  120]  batch loss:0.086799\n",
            "Progress:[   56/  120]  batch loss:0.048724\n",
            "Progress:[   64/  120]  batch loss:0.583615\n",
            "Progress:[   72/  120]  batch loss:0.008339\n",
            "Progress:[   80/  120]  batch loss:0.178625\n",
            "Progress:[   88/  120]  batch loss:0.097067\n",
            "Progress:[   96/  120]  batch loss:0.009189\n",
            "Progress:[  104/  120]  batch loss:0.075380\n",
            "Progress:[  112/  120]  batch loss:0.021047\n",
            "Progress:[  120/  120]  batch loss:0.035164\n",
            "Train Error: Avg loss: 0.087317\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.119034\n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.007364\n",
            "Progress:[   16/  120]  batch loss:0.002323\n",
            "Progress:[   24/  120]  batch loss:0.118132\n",
            "Progress:[   32/  120]  batch loss:0.032847\n",
            "Progress:[   40/  120]  batch loss:0.003552\n",
            "Progress:[   48/  120]  batch loss:0.086146\n",
            "Progress:[   56/  120]  batch loss:0.046592\n",
            "Progress:[   64/  120]  batch loss:0.568805\n",
            "Progress:[   72/  120]  batch loss:0.008458\n",
            "Progress:[   80/  120]  batch loss:0.171804\n",
            "Progress:[   88/  120]  batch loss:0.100458\n",
            "Progress:[   96/  120]  batch loss:0.008706\n",
            "Progress:[  104/  120]  batch loss:0.075812\n",
            "Progress:[  112/  120]  batch loss:0.020508\n",
            "Progress:[  120/  120]  batch loss:0.034351\n",
            "Train Error: Avg loss: 0.085724\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.115475\n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006976\n",
            "Progress:[   16/  120]  batch loss:0.002170\n",
            "Progress:[   24/  120]  batch loss:0.113609\n",
            "Progress:[   32/  120]  batch loss:0.030675\n",
            "Progress:[   40/  120]  batch loss:0.003414\n",
            "Progress:[   48/  120]  batch loss:0.084425\n",
            "Progress:[   56/  120]  batch loss:0.047954\n",
            "Progress:[   64/  120]  batch loss:0.574859\n",
            "Progress:[   72/  120]  batch loss:0.008142\n",
            "Progress:[   80/  120]  batch loss:0.171514\n",
            "Progress:[   88/  120]  batch loss:0.101006\n",
            "Progress:[   96/  120]  batch loss:0.008197\n",
            "Progress:[  104/  120]  batch loss:0.077161\n",
            "Progress:[  112/  120]  batch loss:0.020625\n",
            "Progress:[  120/  120]  batch loss:0.034707\n",
            "Train Error: Avg loss: 0.085696\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.119315\n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006856\n",
            "Progress:[   16/  120]  batch loss:0.002142\n",
            "Progress:[   24/  120]  batch loss:0.116833\n",
            "Progress:[   32/  120]  batch loss:0.030847\n",
            "Progress:[   40/  120]  batch loss:0.003281\n",
            "Progress:[   48/  120]  batch loss:0.083868\n",
            "Progress:[   56/  120]  batch loss:0.048371\n",
            "Progress:[   64/  120]  batch loss:0.577923\n",
            "Progress:[   72/  120]  batch loss:0.007888\n",
            "Progress:[   80/  120]  batch loss:0.175260\n",
            "Progress:[   88/  120]  batch loss:0.095701\n",
            "Progress:[   96/  120]  batch loss:0.008183\n",
            "Progress:[  104/  120]  batch loss:0.074921\n",
            "Progress:[  112/  120]  batch loss:0.020368\n",
            "Progress:[  120/  120]  batch loss:0.034913\n",
            "Train Error: Avg loss: 0.085824\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.122111\n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006909\n",
            "Progress:[   16/  120]  batch loss:0.002145\n",
            "Progress:[   24/  120]  batch loss:0.121076\n",
            "Progress:[   32/  120]  batch loss:0.031916\n",
            "Progress:[   40/  120]  batch loss:0.003117\n",
            "Progress:[   48/  120]  batch loss:0.083628\n",
            "Progress:[   56/  120]  batch loss:0.046398\n",
            "Progress:[   64/  120]  batch loss:0.575977\n",
            "Progress:[   72/  120]  batch loss:0.007683\n",
            "Progress:[   80/  120]  batch loss:0.176777\n",
            "Progress:[   88/  120]  batch loss:0.091915\n",
            "Progress:[   96/  120]  batch loss:0.008280\n",
            "Progress:[  104/  120]  batch loss:0.071379\n",
            "Progress:[  112/  120]  batch loss:0.019381\n",
            "Progress:[  120/  120]  batch loss:0.033535\n",
            "Train Error: Avg loss: 0.085341\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.118467\n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006733\n",
            "Progress:[   16/  120]  batch loss:0.002083\n",
            "Progress:[   24/  120]  batch loss:0.120482\n",
            "Progress:[   32/  120]  batch loss:0.032144\n",
            "Progress:[   40/  120]  batch loss:0.002991\n",
            "Progress:[   48/  120]  batch loss:0.083437\n",
            "Progress:[   56/  120]  batch loss:0.044062\n",
            "Progress:[   64/  120]  batch loss:0.570879\n",
            "Progress:[   72/  120]  batch loss:0.007497\n",
            "Progress:[   80/  120]  batch loss:0.175576\n",
            "Progress:[   88/  120]  batch loss:0.090676\n",
            "Progress:[   96/  120]  batch loss:0.007981\n",
            "Progress:[  104/  120]  batch loss:0.069835\n",
            "Progress:[  112/  120]  batch loss:0.018616\n",
            "Progress:[  120/  120]  batch loss:0.032862\n",
            "Train Error: Avg loss: 0.084390\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.116551\n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006536\n",
            "Progress:[   16/  120]  batch loss:0.002001\n",
            "Progress:[   24/  120]  batch loss:0.117943\n",
            "Progress:[   32/  120]  batch loss:0.031761\n",
            "Progress:[   40/  120]  batch loss:0.002897\n",
            "Progress:[   48/  120]  batch loss:0.082559\n",
            "Progress:[   56/  120]  batch loss:0.044130\n",
            "Progress:[   64/  120]  batch loss:0.570358\n",
            "Progress:[   72/  120]  batch loss:0.007406\n",
            "Progress:[   80/  120]  batch loss:0.177872\n",
            "Progress:[   88/  120]  batch loss:0.089776\n",
            "Progress:[   96/  120]  batch loss:0.007673\n",
            "Progress:[  104/  120]  batch loss:0.070316\n",
            "Progress:[  112/  120]  batch loss:0.018873\n",
            "Progress:[  120/  120]  batch loss:0.032966\n",
            "Train Error: Avg loss: 0.084204\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.120415\n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006434\n",
            "Progress:[   16/  120]  batch loss:0.001999\n",
            "Progress:[   24/  120]  batch loss:0.121612\n",
            "Progress:[   32/  120]  batch loss:0.031828\n",
            "Progress:[   40/  120]  batch loss:0.002789\n",
            "Progress:[   48/  120]  batch loss:0.082299\n",
            "Progress:[   56/  120]  batch loss:0.043390\n",
            "Progress:[   64/  120]  batch loss:0.571741\n",
            "Progress:[   72/  120]  batch loss:0.007218\n",
            "Progress:[   80/  120]  batch loss:0.177124\n",
            "Progress:[   88/  120]  batch loss:0.087388\n",
            "Progress:[   96/  120]  batch loss:0.007674\n",
            "Progress:[  104/  120]  batch loss:0.068337\n",
            "Progress:[  112/  120]  batch loss:0.018128\n",
            "Progress:[  120/  120]  batch loss:0.031983\n",
            "Train Error: Avg loss: 0.083996\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.116362\n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006506\n",
            "Progress:[   16/  120]  batch loss:0.001961\n",
            "Progress:[   24/  120]  batch loss:0.120638\n",
            "Progress:[   32/  120]  batch loss:0.032218\n",
            "Progress:[   40/  120]  batch loss:0.002715\n",
            "Progress:[   48/  120]  batch loss:0.081412\n",
            "Progress:[   56/  120]  batch loss:0.042920\n",
            "Progress:[   64/  120]  batch loss:0.568859\n",
            "Progress:[   72/  120]  batch loss:0.007152\n",
            "Progress:[   80/  120]  batch loss:0.176474\n",
            "Progress:[   88/  120]  batch loss:0.085127\n",
            "Progress:[   96/  120]  batch loss:0.007666\n",
            "Progress:[  104/  120]  batch loss:0.066629\n",
            "Progress:[  112/  120]  batch loss:0.018281\n",
            "Progress:[  120/  120]  batch loss:0.032164\n",
            "Train Error: Avg loss: 0.083381\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.116113\n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006335\n",
            "Progress:[   16/  120]  batch loss:0.001955\n",
            "Progress:[   24/  120]  batch loss:0.119753\n",
            "Progress:[   32/  120]  batch loss:0.031689\n",
            "Progress:[   40/  120]  batch loss:0.002685\n",
            "Progress:[   48/  120]  batch loss:0.081765\n",
            "Progress:[   56/  120]  batch loss:0.042015\n",
            "Progress:[   64/  120]  batch loss:0.561686\n",
            "Progress:[   72/  120]  batch loss:0.007184\n",
            "Progress:[   80/  120]  batch loss:0.174557\n",
            "Progress:[   88/  120]  batch loss:0.084740\n",
            "Progress:[   96/  120]  batch loss:0.007471\n",
            "Progress:[  104/  120]  batch loss:0.067429\n",
            "Progress:[  112/  120]  batch loss:0.018000\n",
            "Progress:[  120/  120]  batch loss:0.031331\n",
            "Train Error: Avg loss: 0.082573\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.112447\n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006242\n",
            "Progress:[   16/  120]  batch loss:0.001884\n",
            "Progress:[   24/  120]  batch loss:0.118192\n",
            "Progress:[   32/  120]  batch loss:0.031820\n",
            "Progress:[   40/  120]  batch loss:0.002572\n",
            "Progress:[   48/  120]  batch loss:0.081007\n",
            "Progress:[   56/  120]  batch loss:0.040837\n",
            "Progress:[   64/  120]  batch loss:0.562794\n",
            "Progress:[   72/  120]  batch loss:0.006895\n",
            "Progress:[   80/  120]  batch loss:0.175401\n",
            "Progress:[   88/  120]  batch loss:0.085166\n",
            "Progress:[   96/  120]  batch loss:0.007166\n",
            "Progress:[  104/  120]  batch loss:0.064458\n",
            "Progress:[  112/  120]  batch loss:0.017173\n",
            "Progress:[  120/  120]  batch loss:0.030441\n",
            "Train Error: Avg loss: 0.082137\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.113725\n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006042\n",
            "Progress:[   16/  120]  batch loss:0.001827\n",
            "Progress:[   24/  120]  batch loss:0.117356\n",
            "Progress:[   32/  120]  batch loss:0.031137\n",
            "Progress:[   40/  120]  batch loss:0.002471\n",
            "Progress:[   48/  120]  batch loss:0.080655\n",
            "Progress:[   56/  120]  batch loss:0.039960\n",
            "Progress:[   64/  120]  batch loss:0.561420\n",
            "Progress:[   72/  120]  batch loss:0.006815\n",
            "Progress:[   80/  120]  batch loss:0.172378\n",
            "Progress:[   88/  120]  batch loss:0.083629\n",
            "Progress:[   96/  120]  batch loss:0.007091\n",
            "Progress:[  104/  120]  batch loss:0.064782\n",
            "Progress:[  112/  120]  batch loss:0.016914\n",
            "Progress:[  120/  120]  batch loss:0.030034\n",
            "Train Error: Avg loss: 0.081501\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.109693\n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006059\n",
            "Progress:[   16/  120]  batch loss:0.001791\n",
            "Progress:[   24/  120]  batch loss:0.113969\n",
            "Progress:[   32/  120]  batch loss:0.031650\n",
            "Progress:[   40/  120]  batch loss:0.002406\n",
            "Progress:[   48/  120]  batch loss:0.080695\n",
            "Progress:[   56/  120]  batch loss:0.038215\n",
            "Progress:[   64/  120]  batch loss:0.557394\n",
            "Progress:[   72/  120]  batch loss:0.006657\n",
            "Progress:[   80/  120]  batch loss:0.171985\n",
            "Progress:[   88/  120]  batch loss:0.085951\n",
            "Progress:[   96/  120]  batch loss:0.006667\n",
            "Progress:[  104/  120]  batch loss:0.064349\n",
            "Progress:[  112/  120]  batch loss:0.015997\n",
            "Progress:[  120/  120]  batch loss:0.028833\n",
            "Train Error: Avg loss: 0.080841\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.109709\n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005635\n",
            "Progress:[   16/  120]  batch loss:0.001681\n",
            "Progress:[   24/  120]  batch loss:0.115312\n",
            "Progress:[   32/  120]  batch loss:0.030104\n",
            "Progress:[   40/  120]  batch loss:0.002315\n",
            "Progress:[   48/  120]  batch loss:0.078293\n",
            "Progress:[   56/  120]  batch loss:0.040866\n",
            "Progress:[   64/  120]  batch loss:0.565529\n",
            "Progress:[   72/  120]  batch loss:0.006586\n",
            "Progress:[   80/  120]  batch loss:0.172452\n",
            "Progress:[   88/  120]  batch loss:0.081484\n",
            "Progress:[   96/  120]  batch loss:0.006667\n",
            "Progress:[  104/  120]  batch loss:0.065568\n",
            "Progress:[  112/  120]  batch loss:0.017103\n",
            "Progress:[  120/  120]  batch loss:0.030246\n",
            "Train Error: Avg loss: 0.081323\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.113309\n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006024\n",
            "Progress:[   16/  120]  batch loss:0.001780\n",
            "Progress:[   24/  120]  batch loss:0.120163\n",
            "Progress:[   32/  120]  batch loss:0.031563\n",
            "Progress:[   40/  120]  batch loss:0.002314\n",
            "Progress:[   48/  120]  batch loss:0.080148\n",
            "Progress:[   56/  120]  batch loss:0.039019\n",
            "Progress:[   64/  120]  batch loss:0.556971\n",
            "Progress:[   72/  120]  batch loss:0.006546\n",
            "Progress:[   80/  120]  batch loss:0.173287\n",
            "Progress:[   88/  120]  batch loss:0.079537\n",
            "Progress:[   96/  120]  batch loss:0.006757\n",
            "Progress:[  104/  120]  batch loss:0.061813\n",
            "Progress:[  112/  120]  batch loss:0.016009\n",
            "Progress:[  120/  120]  batch loss:0.028807\n",
            "Train Error: Avg loss: 0.080716\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.106807\n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005927\n",
            "Progress:[   16/  120]  batch loss:0.001748\n",
            "Progress:[   24/  120]  batch loss:0.116514\n",
            "Progress:[   32/  120]  batch loss:0.031618\n",
            "Progress:[   40/  120]  batch loss:0.002226\n",
            "Progress:[   48/  120]  batch loss:0.077831\n",
            "Progress:[   56/  120]  batch loss:0.038680\n",
            "Progress:[   64/  120]  batch loss:0.553483\n",
            "Progress:[   72/  120]  batch loss:0.006544\n",
            "Progress:[   80/  120]  batch loss:0.164316\n",
            "Progress:[   88/  120]  batch loss:0.084713\n",
            "Progress:[   96/  120]  batch loss:0.006136\n",
            "Progress:[  104/  120]  batch loss:0.066511\n",
            "Progress:[  112/  120]  batch loss:0.015817\n",
            "Progress:[  120/  120]  batch loss:0.027754\n",
            "Train Error: Avg loss: 0.079988\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.103988\n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005827\n",
            "Progress:[   16/  120]  batch loss:0.001653\n",
            "Progress:[   24/  120]  batch loss:0.114744\n",
            "Progress:[   32/  120]  batch loss:0.030896\n",
            "Progress:[   40/  120]  batch loss:0.002088\n",
            "Progress:[   48/  120]  batch loss:0.077122\n",
            "Progress:[   56/  120]  batch loss:0.038004\n",
            "Progress:[   64/  120]  batch loss:0.560216\n",
            "Progress:[   72/  120]  batch loss:0.006050\n",
            "Progress:[   80/  120]  batch loss:0.171826\n",
            "Progress:[   88/  120]  batch loss:0.083377\n",
            "Progress:[   96/  120]  batch loss:0.005839\n",
            "Progress:[  104/  120]  batch loss:0.062265\n",
            "Progress:[  112/  120]  batch loss:0.014895\n",
            "Progress:[  120/  120]  batch loss:0.027150\n",
            "Train Error: Avg loss: 0.080130\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.109452\n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005442\n",
            "Progress:[   16/  120]  batch loss:0.001610\n",
            "Progress:[   24/  120]  batch loss:0.118614\n",
            "Progress:[   32/  120]  batch loss:0.029871\n",
            "Progress:[   40/  120]  batch loss:0.002049\n",
            "Progress:[   48/  120]  batch loss:0.075239\n",
            "Progress:[   56/  120]  batch loss:0.042926\n",
            "Progress:[   64/  120]  batch loss:0.570629\n",
            "Progress:[   72/  120]  batch loss:0.006209\n",
            "Progress:[   80/  120]  batch loss:0.168279\n",
            "Progress:[   88/  120]  batch loss:0.081800\n",
            "Progress:[   96/  120]  batch loss:0.005914\n",
            "Progress:[  104/  120]  batch loss:0.067017\n",
            "Progress:[  112/  120]  batch loss:0.016732\n",
            "Progress:[  120/  120]  batch loss:0.029641\n",
            "Train Error: Avg loss: 0.081465\n",
            "Test Error: Accuracy: 90.0%, Avg loss: 0.109928\n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.006099\n",
            "Progress:[   16/  120]  batch loss:0.001748\n",
            "Progress:[   24/  120]  batch loss:0.118846\n",
            "Progress:[   32/  120]  batch loss:0.031771\n",
            "Progress:[   40/  120]  batch loss:0.002127\n",
            "Progress:[   48/  120]  batch loss:0.075828\n",
            "Progress:[   56/  120]  batch loss:0.041085\n",
            "Progress:[   64/  120]  batch loss:0.554982\n",
            "Progress:[   72/  120]  batch loss:0.006488\n",
            "Progress:[   80/  120]  batch loss:0.166423\n",
            "Progress:[   88/  120]  batch loss:0.078212\n",
            "Progress:[   96/  120]  batch loss:0.006166\n",
            "Progress:[  104/  120]  batch loss:0.065225\n",
            "Progress:[  112/  120]  batch loss:0.016139\n",
            "Progress:[  120/  120]  batch loss:0.028855\n",
            "Train Error: Avg loss: 0.079999\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.107260\n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005927\n",
            "Progress:[   16/  120]  batch loss:0.001688\n",
            "Progress:[   24/  120]  batch loss:0.118413\n",
            "Progress:[   32/  120]  batch loss:0.031611\n",
            "Progress:[   40/  120]  batch loss:0.002002\n",
            "Progress:[   48/  120]  batch loss:0.077937\n",
            "Progress:[   56/  120]  batch loss:0.036954\n",
            "Progress:[   64/  120]  batch loss:0.552502\n",
            "Progress:[   72/  120]  batch loss:0.006060\n",
            "Progress:[   80/  120]  batch loss:0.171312\n",
            "Progress:[   88/  120]  batch loss:0.075141\n",
            "Progress:[   96/  120]  batch loss:0.006076\n",
            "Progress:[  104/  120]  batch loss:0.058622\n",
            "Progress:[  112/  120]  batch loss:0.014446\n",
            "Progress:[  120/  120]  batch loss:0.026555\n",
            "Train Error: Avg loss: 0.079016\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.104122\n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005533\n",
            "Progress:[   16/  120]  batch loss:0.001596\n",
            "Progress:[   24/  120]  batch loss:0.116004\n",
            "Progress:[   32/  120]  batch loss:0.031043\n",
            "Progress:[   40/  120]  batch loss:0.001949\n",
            "Progress:[   48/  120]  batch loss:0.076790\n",
            "Progress:[   56/  120]  batch loss:0.036530\n",
            "Progress:[   64/  120]  batch loss:0.547230\n",
            "Progress:[   72/  120]  batch loss:0.006212\n",
            "Progress:[   80/  120]  batch loss:0.163996\n",
            "Progress:[   88/  120]  batch loss:0.077666\n",
            "Progress:[   96/  120]  batch loss:0.005836\n",
            "Progress:[  104/  120]  batch loss:0.062009\n",
            "Progress:[  112/  120]  batch loss:0.014815\n",
            "Progress:[  120/  120]  batch loss:0.026348\n",
            "Train Error: Avg loss: 0.078237\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.100690\n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005630\n",
            "Progress:[   16/  120]  batch loss:0.001578\n",
            "Progress:[   24/  120]  batch loss:0.112981\n",
            "Progress:[   32/  120]  batch loss:0.030880\n",
            "Progress:[   40/  120]  batch loss:0.001921\n",
            "Progress:[   48/  120]  batch loss:0.075223\n",
            "Progress:[   56/  120]  batch loss:0.037378\n",
            "Progress:[   64/  120]  batch loss:0.546600\n",
            "Progress:[   72/  120]  batch loss:0.006174\n",
            "Progress:[   80/  120]  batch loss:0.164379\n",
            "Progress:[   88/  120]  batch loss:0.077099\n",
            "Progress:[   96/  120]  batch loss:0.005625\n",
            "Progress:[  104/  120]  batch loss:0.062397\n",
            "Progress:[  112/  120]  batch loss:0.015025\n",
            "Progress:[  120/  120]  batch loss:0.026592\n",
            "Train Error: Avg loss: 0.077966\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.106700\n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005549\n",
            "Progress:[   16/  120]  batch loss:0.001584\n",
            "Progress:[   24/  120]  batch loss:0.118577\n",
            "Progress:[   32/  120]  batch loss:0.030660\n",
            "Progress:[   40/  120]  batch loss:0.001859\n",
            "Progress:[   48/  120]  batch loss:0.075834\n",
            "Progress:[   56/  120]  batch loss:0.038036\n",
            "Progress:[   64/  120]  batch loss:0.551446\n",
            "Progress:[   72/  120]  batch loss:0.005982\n",
            "Progress:[   80/  120]  batch loss:0.169312\n",
            "Progress:[   88/  120]  batch loss:0.072667\n",
            "Progress:[   96/  120]  batch loss:0.005812\n",
            "Progress:[  104/  120]  batch loss:0.060477\n",
            "Progress:[  112/  120]  batch loss:0.014714\n",
            "Progress:[  120/  120]  batch loss:0.026665\n",
            "Train Error: Avg loss: 0.078612\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.106964\n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005644\n",
            "Progress:[   16/  120]  batch loss:0.001597\n",
            "Progress:[   24/  120]  batch loss:0.119499\n",
            "Progress:[   32/  120]  batch loss:0.032047\n",
            "Progress:[   40/  120]  batch loss:0.001829\n",
            "Progress:[   48/  120]  batch loss:0.075824\n",
            "Progress:[   56/  120]  batch loss:0.036508\n",
            "Progress:[   64/  120]  batch loss:0.547358\n",
            "Progress:[   72/  120]  batch loss:0.006094\n",
            "Progress:[   80/  120]  batch loss:0.164188\n",
            "Progress:[   88/  120]  batch loss:0.072030\n",
            "Progress:[   96/  120]  batch loss:0.005862\n",
            "Progress:[  104/  120]  batch loss:0.061296\n",
            "Progress:[  112/  120]  batch loss:0.014425\n",
            "Progress:[  120/  120]  batch loss:0.025917\n",
            "Train Error: Avg loss: 0.078008\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.101237\n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005672\n",
            "Progress:[   16/  120]  batch loss:0.001560\n",
            "Progress:[   24/  120]  batch loss:0.115945\n",
            "Progress:[   32/  120]  batch loss:0.031769\n",
            "Progress:[   40/  120]  batch loss:0.001776\n",
            "Progress:[   48/  120]  batch loss:0.076823\n",
            "Progress:[   56/  120]  batch loss:0.034303\n",
            "Progress:[   64/  120]  batch loss:0.542862\n",
            "Progress:[   72/  120]  batch loss:0.005854\n",
            "Progress:[   80/  120]  batch loss:0.166900\n",
            "Progress:[   88/  120]  batch loss:0.071283\n",
            "Progress:[   96/  120]  batch loss:0.005609\n",
            "Progress:[  104/  120]  batch loss:0.057228\n",
            "Progress:[  112/  120]  batch loss:0.013595\n",
            "Progress:[  120/  120]  batch loss:0.024711\n",
            "Train Error: Avg loss: 0.077059\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.102885\n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005292\n",
            "Progress:[   16/  120]  batch loss:0.001506\n",
            "Progress:[   24/  120]  batch loss:0.114835\n",
            "Progress:[   32/  120]  batch loss:0.030460\n",
            "Progress:[   40/  120]  batch loss:0.001726\n",
            "Progress:[   48/  120]  batch loss:0.077244\n",
            "Progress:[   56/  120]  batch loss:0.034199\n",
            "Progress:[   64/  120]  batch loss:0.540589\n",
            "Progress:[   72/  120]  batch loss:0.005862\n",
            "Progress:[   80/  120]  batch loss:0.166128\n",
            "Progress:[   88/  120]  batch loss:0.069887\n",
            "Progress:[   96/  120]  batch loss:0.005633\n",
            "Progress:[  104/  120]  batch loss:0.057899\n",
            "Progress:[  112/  120]  batch loss:0.013759\n",
            "Progress:[  120/  120]  batch loss:0.025164\n",
            "Train Error: Avg loss: 0.076679\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.101735\n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005386\n",
            "Progress:[   16/  120]  batch loss:0.001511\n",
            "Progress:[   24/  120]  batch loss:0.114351\n",
            "Progress:[   32/  120]  batch loss:0.031034\n",
            "Progress:[   40/  120]  batch loss:0.001697\n",
            "Progress:[   48/  120]  batch loss:0.077435\n",
            "Progress:[   56/  120]  batch loss:0.032956\n",
            "Progress:[   64/  120]  batch loss:0.537387\n",
            "Progress:[   72/  120]  batch loss:0.005812\n",
            "Progress:[   80/  120]  batch loss:0.164303\n",
            "Progress:[   88/  120]  batch loss:0.070944\n",
            "Progress:[   96/  120]  batch loss:0.005474\n",
            "Progress:[  104/  120]  batch loss:0.057582\n",
            "Progress:[  112/  120]  batch loss:0.013106\n",
            "Progress:[  120/  120]  batch loss:0.024054\n",
            "Train Error: Avg loss: 0.076202\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.098376\n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005154\n",
            "Progress:[   16/  120]  batch loss:0.001439\n",
            "Progress:[   24/  120]  batch loss:0.111477\n",
            "Progress:[   32/  120]  batch loss:0.030677\n",
            "Progress:[   40/  120]  batch loss:0.001629\n",
            "Progress:[   48/  120]  batch loss:0.075919\n",
            "Progress:[   56/  120]  batch loss:0.033196\n",
            "Progress:[   64/  120]  batch loss:0.540079\n",
            "Progress:[   72/  120]  batch loss:0.005716\n",
            "Progress:[   80/  120]  batch loss:0.164226\n",
            "Progress:[   88/  120]  batch loss:0.071742\n",
            "Progress:[   96/  120]  batch loss:0.005328\n",
            "Progress:[  104/  120]  batch loss:0.056968\n",
            "Progress:[  112/  120]  batch loss:0.013108\n",
            "Progress:[  120/  120]  batch loss:0.024604\n",
            "Train Error: Avg loss: 0.076084\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.100789\n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005237\n",
            "Progress:[   16/  120]  batch loss:0.001451\n",
            "Progress:[   24/  120]  batch loss:0.115170\n",
            "Progress:[   32/  120]  batch loss:0.030108\n",
            "Progress:[   40/  120]  batch loss:0.001617\n",
            "Progress:[   48/  120]  batch loss:0.076237\n",
            "Progress:[   56/  120]  batch loss:0.033730\n",
            "Progress:[   64/  120]  batch loss:0.538564\n",
            "Progress:[   72/  120]  batch loss:0.005694\n",
            "Progress:[   80/  120]  batch loss:0.166008\n",
            "Progress:[   88/  120]  batch loss:0.069019\n",
            "Progress:[   96/  120]  batch loss:0.005424\n",
            "Progress:[  104/  120]  batch loss:0.055766\n",
            "Progress:[  112/  120]  batch loss:0.013218\n",
            "Progress:[  120/  120]  batch loss:0.023819\n",
            "Train Error: Avg loss: 0.076071\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.095878\n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005197\n",
            "Progress:[   16/  120]  batch loss:0.001466\n",
            "Progress:[   24/  120]  batch loss:0.114350\n",
            "Progress:[   32/  120]  batch loss:0.030595\n",
            "Progress:[   40/  120]  batch loss:0.001644\n",
            "Progress:[   48/  120]  batch loss:0.075031\n",
            "Progress:[   56/  120]  batch loss:0.033471\n",
            "Progress:[   64/  120]  batch loss:0.530223\n",
            "Progress:[   72/  120]  batch loss:0.005958\n",
            "Progress:[   80/  120]  batch loss:0.158566\n",
            "Progress:[   88/  120]  batch loss:0.069279\n",
            "Progress:[   96/  120]  batch loss:0.005220\n",
            "Progress:[  104/  120]  batch loss:0.059064\n",
            "Progress:[  112/  120]  batch loss:0.013506\n",
            "Progress:[  120/  120]  batch loss:0.023584\n",
            "Train Error: Avg loss: 0.075144\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.096437\n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005312\n",
            "Progress:[   16/  120]  batch loss:0.001453\n",
            "Progress:[   24/  120]  batch loss:0.110741\n",
            "Progress:[   32/  120]  batch loss:0.030474\n",
            "Progress:[   40/  120]  batch loss:0.001577\n",
            "Progress:[   48/  120]  batch loss:0.073994\n",
            "Progress:[   56/  120]  batch loss:0.033304\n",
            "Progress:[   64/  120]  batch loss:0.533986\n",
            "Progress:[   72/  120]  batch loss:0.005801\n",
            "Progress:[   80/  120]  batch loss:0.159400\n",
            "Progress:[   88/  120]  batch loss:0.069174\n",
            "Progress:[   96/  120]  batch loss:0.005167\n",
            "Progress:[  104/  120]  batch loss:0.058012\n",
            "Progress:[  112/  120]  batch loss:0.013112\n",
            "Progress:[  120/  120]  batch loss:0.023543\n",
            "Train Error: Avg loss: 0.075003\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.093787\n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005253\n",
            "Progress:[   16/  120]  batch loss:0.001434\n",
            "Progress:[   24/  120]  batch loss:0.109832\n",
            "Progress:[   32/  120]  batch loss:0.030436\n",
            "Progress:[   40/  120]  batch loss:0.001531\n",
            "Progress:[   48/  120]  batch loss:0.078541\n",
            "Progress:[   56/  120]  batch loss:0.029958\n",
            "Progress:[   64/  120]  batch loss:0.526708\n",
            "Progress:[   72/  120]  batch loss:0.005524\n",
            "Progress:[   80/  120]  batch loss:0.165883\n",
            "Progress:[   88/  120]  batch loss:0.066950\n",
            "Progress:[   96/  120]  batch loss:0.005121\n",
            "Progress:[  104/  120]  batch loss:0.051508\n",
            "Progress:[  112/  120]  batch loss:0.012007\n",
            "Progress:[  120/  120]  batch loss:0.021513\n",
            "Train Error: Avg loss: 0.074146\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.093255\n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004744\n",
            "Progress:[   16/  120]  batch loss:0.001364\n",
            "Progress:[   24/  120]  batch loss:0.108499\n",
            "Progress:[   32/  120]  batch loss:0.028566\n",
            "Progress:[   40/  120]  batch loss:0.001520\n",
            "Progress:[   48/  120]  batch loss:0.073623\n",
            "Progress:[   56/  120]  batch loss:0.033668\n",
            "Progress:[   64/  120]  batch loss:0.525999\n",
            "Progress:[   72/  120]  batch loss:0.005948\n",
            "Progress:[   80/  120]  batch loss:0.155825\n",
            "Progress:[   88/  120]  batch loss:0.070554\n",
            "Progress:[   96/  120]  batch loss:0.005014\n",
            "Progress:[  104/  120]  batch loss:0.059965\n",
            "Progress:[  112/  120]  batch loss:0.013818\n",
            "Progress:[  120/  120]  batch loss:0.023750\n",
            "Train Error: Avg loss: 0.074191\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.094227\n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005311\n",
            "Progress:[   16/  120]  batch loss:0.001455\n",
            "Progress:[   24/  120]  batch loss:0.111110\n",
            "Progress:[   32/  120]  batch loss:0.030206\n",
            "Progress:[   40/  120]  batch loss:0.001489\n",
            "Progress:[   48/  120]  batch loss:0.075340\n",
            "Progress:[   56/  120]  batch loss:0.031134\n",
            "Progress:[   64/  120]  batch loss:0.526224\n",
            "Progress:[   72/  120]  batch loss:0.005595\n",
            "Progress:[   80/  120]  batch loss:0.161776\n",
            "Progress:[   88/  120]  batch loss:0.068058\n",
            "Progress:[   96/  120]  batch loss:0.004879\n",
            "Progress:[  104/  120]  batch loss:0.054293\n",
            "Progress:[  112/  120]  batch loss:0.012142\n",
            "Progress:[  120/  120]  batch loss:0.021271\n",
            "Train Error: Avg loss: 0.074019\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.092292\n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004804\n",
            "Progress:[   16/  120]  batch loss:0.001349\n",
            "Progress:[   24/  120]  batch loss:0.109966\n",
            "Progress:[   32/  120]  batch loss:0.029046\n",
            "Progress:[   40/  120]  batch loss:0.001426\n",
            "Progress:[   48/  120]  batch loss:0.072514\n",
            "Progress:[   56/  120]  batch loss:0.032675\n",
            "Progress:[   64/  120]  batch loss:0.528912\n",
            "Progress:[   72/  120]  batch loss:0.005721\n",
            "Progress:[   80/  120]  batch loss:0.157466\n",
            "Progress:[   88/  120]  batch loss:0.067013\n",
            "Progress:[   96/  120]  batch loss:0.004870\n",
            "Progress:[  104/  120]  batch loss:0.057764\n",
            "Progress:[  112/  120]  batch loss:0.013144\n",
            "Progress:[  120/  120]  batch loss:0.022516\n",
            "Train Error: Avg loss: 0.073946\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.090392\n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005234\n",
            "Progress:[   16/  120]  batch loss:0.001433\n",
            "Progress:[   24/  120]  batch loss:0.109824\n",
            "Progress:[   32/  120]  batch loss:0.030452\n",
            "Progress:[   40/  120]  batch loss:0.001471\n",
            "Progress:[   48/  120]  batch loss:0.072064\n",
            "Progress:[   56/  120]  batch loss:0.032923\n",
            "Progress:[   64/  120]  batch loss:0.521922\n",
            "Progress:[   72/  120]  batch loss:0.005975\n",
            "Progress:[   80/  120]  batch loss:0.152434\n",
            "Progress:[   88/  120]  batch loss:0.068565\n",
            "Progress:[   96/  120]  batch loss:0.004707\n",
            "Progress:[  104/  120]  batch loss:0.058949\n",
            "Progress:[  112/  120]  batch loss:0.013068\n",
            "Progress:[  120/  120]  batch loss:0.022496\n",
            "Train Error: Avg loss: 0.073434\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.092992\n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005162\n",
            "Progress:[   16/  120]  batch loss:0.001384\n",
            "Progress:[   24/  120]  batch loss:0.110897\n",
            "Progress:[   32/  120]  batch loss:0.029339\n",
            "Progress:[   40/  120]  batch loss:0.001378\n",
            "Progress:[   48/  120]  batch loss:0.073622\n",
            "Progress:[   56/  120]  batch loss:0.031798\n",
            "Progress:[   64/  120]  batch loss:0.530407\n",
            "Progress:[   72/  120]  batch loss:0.005519\n",
            "Progress:[   80/  120]  batch loss:0.159955\n",
            "Progress:[   88/  120]  batch loss:0.065331\n",
            "Progress:[   96/  120]  batch loss:0.004703\n",
            "Progress:[  104/  120]  batch loss:0.055275\n",
            "Progress:[  112/  120]  batch loss:0.012044\n",
            "Progress:[  120/  120]  batch loss:0.021401\n",
            "Train Error: Avg loss: 0.073881\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.090119\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004966\n",
            "Progress:[   16/  120]  batch loss:0.001350\n",
            "Progress:[   24/  120]  batch loss:0.109514\n",
            "Progress:[   32/  120]  batch loss:0.029908\n",
            "Progress:[   40/  120]  batch loss:0.001365\n",
            "Progress:[   48/  120]  batch loss:0.074293\n",
            "Progress:[   56/  120]  batch loss:0.030553\n",
            "Progress:[   64/  120]  batch loss:0.521330\n",
            "Progress:[   72/  120]  batch loss:0.005625\n",
            "Progress:[   80/  120]  batch loss:0.158901\n",
            "Progress:[   88/  120]  batch loss:0.063002\n",
            "Progress:[   96/  120]  batch loss:0.004821\n",
            "Progress:[  104/  120]  batch loss:0.053748\n",
            "Progress:[  112/  120]  batch loss:0.012299\n",
            "Progress:[  120/  120]  batch loss:0.020923\n",
            "Train Error: Avg loss: 0.072840\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.086868\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004889\n",
            "Progress:[   16/  120]  batch loss:0.001370\n",
            "Progress:[   24/  120]  batch loss:0.106110\n",
            "Progress:[   32/  120]  batch loss:0.029279\n",
            "Progress:[   40/  120]  batch loss:0.001392\n",
            "Progress:[   48/  120]  batch loss:0.076998\n",
            "Progress:[   56/  120]  batch loss:0.028881\n",
            "Progress:[   64/  120]  batch loss:0.508222\n",
            "Progress:[   72/  120]  batch loss:0.005788\n",
            "Progress:[   80/  120]  batch loss:0.156337\n",
            "Progress:[   88/  120]  batch loss:0.064806\n",
            "Progress:[   96/  120]  batch loss:0.004610\n",
            "Progress:[  104/  120]  batch loss:0.053107\n",
            "Progress:[  112/  120]  batch loss:0.011986\n",
            "Progress:[  120/  120]  batch loss:0.019898\n",
            "Train Error: Avg loss: 0.071578\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.085379\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004726\n",
            "Progress:[   16/  120]  batch loss:0.001324\n",
            "Progress:[   24/  120]  batch loss:0.104099\n",
            "Progress:[   32/  120]  batch loss:0.028334\n",
            "Progress:[   40/  120]  batch loss:0.001327\n",
            "Progress:[   48/  120]  batch loss:0.070562\n",
            "Progress:[   56/  120]  batch loss:0.031277\n",
            "Progress:[   64/  120]  batch loss:0.517187\n",
            "Progress:[   72/  120]  batch loss:0.005866\n",
            "Progress:[   80/  120]  batch loss:0.151317\n",
            "Progress:[   88/  120]  batch loss:0.066752\n",
            "Progress:[   96/  120]  batch loss:0.004418\n",
            "Progress:[  104/  120]  batch loss:0.057540\n",
            "Progress:[  112/  120]  batch loss:0.012597\n",
            "Progress:[  120/  120]  batch loss:0.020969\n",
            "Train Error: Avg loss: 0.071886\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.086909\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005082\n",
            "Progress:[   16/  120]  batch loss:0.001362\n",
            "Progress:[   24/  120]  batch loss:0.106810\n",
            "Progress:[   32/  120]  batch loss:0.029082\n",
            "Progress:[   40/  120]  batch loss:0.001335\n",
            "Progress:[   48/  120]  batch loss:0.070755\n",
            "Progress:[   56/  120]  batch loss:0.032319\n",
            "Progress:[   64/  120]  batch loss:0.517868\n",
            "Progress:[   72/  120]  batch loss:0.005818\n",
            "Progress:[   80/  120]  batch loss:0.154174\n",
            "Progress:[   88/  120]  batch loss:0.063392\n",
            "Progress:[   96/  120]  batch loss:0.004581\n",
            "Progress:[  104/  120]  batch loss:0.056087\n",
            "Progress:[  112/  120]  batch loss:0.012694\n",
            "Progress:[  120/  120]  batch loss:0.021491\n",
            "Train Error: Avg loss: 0.072190\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.086178\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005058\n",
            "Progress:[   16/  120]  batch loss:0.001383\n",
            "Progress:[   24/  120]  batch loss:0.108194\n",
            "Progress:[   32/  120]  batch loss:0.029615\n",
            "Progress:[   40/  120]  batch loss:0.001336\n",
            "Progress:[   48/  120]  batch loss:0.072674\n",
            "Progress:[   56/  120]  batch loss:0.030321\n",
            "Progress:[   64/  120]  batch loss:0.510813\n",
            "Progress:[   72/  120]  batch loss:0.005879\n",
            "Progress:[   80/  120]  batch loss:0.151754\n",
            "Progress:[   88/  120]  batch loss:0.063638\n",
            "Progress:[   96/  120]  batch loss:0.004512\n",
            "Progress:[  104/  120]  batch loss:0.055091\n",
            "Progress:[  112/  120]  batch loss:0.012064\n",
            "Progress:[  120/  120]  batch loss:0.019909\n",
            "Train Error: Avg loss: 0.071483\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.080675\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004872\n",
            "Progress:[   16/  120]  batch loss:0.001321\n",
            "Progress:[   24/  120]  batch loss:0.103106\n",
            "Progress:[   32/  120]  batch loss:0.028616\n",
            "Progress:[   40/  120]  batch loss:0.001285\n",
            "Progress:[   48/  120]  batch loss:0.069235\n",
            "Progress:[   56/  120]  batch loss:0.030850\n",
            "Progress:[   64/  120]  batch loss:0.510888\n",
            "Progress:[   72/  120]  batch loss:0.005942\n",
            "Progress:[   80/  120]  batch loss:0.147958\n",
            "Progress:[   88/  120]  batch loss:0.065680\n",
            "Progress:[   96/  120]  batch loss:0.004115\n",
            "Progress:[  104/  120]  batch loss:0.057424\n",
            "Progress:[  112/  120]  batch loss:0.012341\n",
            "Progress:[  120/  120]  batch loss:0.020531\n",
            "Train Error: Avg loss: 0.070944\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.086440\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005000\n",
            "Progress:[   16/  120]  batch loss:0.001331\n",
            "Progress:[   24/  120]  batch loss:0.104674\n",
            "Progress:[   32/  120]  batch loss:0.028361\n",
            "Progress:[   40/  120]  batch loss:0.001254\n",
            "Progress:[   48/  120]  batch loss:0.068319\n",
            "Progress:[   56/  120]  batch loss:0.033302\n",
            "Progress:[   64/  120]  batch loss:0.516772\n",
            "Progress:[   72/  120]  batch loss:0.005820\n",
            "Progress:[   80/  120]  batch loss:0.154530\n",
            "Progress:[   88/  120]  batch loss:0.061906\n",
            "Progress:[   96/  120]  batch loss:0.004392\n",
            "Progress:[  104/  120]  batch loss:0.058180\n",
            "Progress:[  112/  120]  batch loss:0.013051\n",
            "Progress:[  120/  120]  batch loss:0.021212\n",
            "Train Error: Avg loss: 0.071874\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.087892\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005231\n",
            "Progress:[   16/  120]  batch loss:0.001410\n",
            "Progress:[   24/  120]  batch loss:0.111012\n",
            "Progress:[   32/  120]  batch loss:0.030016\n",
            "Progress:[   40/  120]  batch loss:0.001255\n",
            "Progress:[   48/  120]  batch loss:0.072716\n",
            "Progress:[   56/  120]  batch loss:0.030246\n",
            "Progress:[   64/  120]  batch loss:0.515916\n",
            "Progress:[   72/  120]  batch loss:0.005665\n",
            "Progress:[   80/  120]  batch loss:0.153129\n",
            "Progress:[   88/  120]  batch loss:0.061451\n",
            "Progress:[   96/  120]  batch loss:0.004343\n",
            "Progress:[  104/  120]  batch loss:0.054200\n",
            "Progress:[  112/  120]  batch loss:0.011807\n",
            "Progress:[  120/  120]  batch loss:0.019715\n",
            "Train Error: Avg loss: 0.071874\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.079836\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004827\n",
            "Progress:[   16/  120]  batch loss:0.001308\n",
            "Progress:[   24/  120]  batch loss:0.101887\n",
            "Progress:[   32/  120]  batch loss:0.028409\n",
            "Progress:[   40/  120]  batch loss:0.001239\n",
            "Progress:[   48/  120]  batch loss:0.069525\n",
            "Progress:[   56/  120]  batch loss:0.030433\n",
            "Progress:[   64/  120]  batch loss:0.505282\n",
            "Progress:[   72/  120]  batch loss:0.005982\n",
            "Progress:[   80/  120]  batch loss:0.146566\n",
            "Progress:[   88/  120]  batch loss:0.063433\n",
            "Progress:[   96/  120]  batch loss:0.004119\n",
            "Progress:[  104/  120]  batch loss:0.056610\n",
            "Progress:[  112/  120]  batch loss:0.012062\n",
            "Progress:[  120/  120]  batch loss:0.019598\n",
            "Train Error: Avg loss: 0.070085\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.081031\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004927\n",
            "Progress:[   16/  120]  batch loss:0.001317\n",
            "Progress:[   24/  120]  batch loss:0.102464\n",
            "Progress:[   32/  120]  batch loss:0.028223\n",
            "Progress:[   40/  120]  batch loss:0.001206\n",
            "Progress:[   48/  120]  batch loss:0.070905\n",
            "Progress:[   56/  120]  batch loss:0.030144\n",
            "Progress:[   64/  120]  batch loss:0.505995\n",
            "Progress:[   72/  120]  batch loss:0.005753\n",
            "Progress:[   80/  120]  batch loss:0.152374\n",
            "Progress:[   88/  120]  batch loss:0.059485\n",
            "Progress:[   96/  120]  batch loss:0.004129\n",
            "Progress:[  104/  120]  batch loss:0.053252\n",
            "Progress:[  112/  120]  batch loss:0.011796\n",
            "Progress:[  120/  120]  batch loss:0.019277\n",
            "Train Error: Avg loss: 0.070083\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.084940\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004794\n",
            "Progress:[   16/  120]  batch loss:0.001323\n",
            "Progress:[   24/  120]  batch loss:0.103488\n",
            "Progress:[   32/  120]  batch loss:0.028499\n",
            "Progress:[   40/  120]  batch loss:0.001173\n",
            "Progress:[   48/  120]  batch loss:0.075179\n",
            "Progress:[   56/  120]  batch loss:0.027079\n",
            "Progress:[   64/  120]  batch loss:0.500745\n",
            "Progress:[   72/  120]  batch loss:0.005772\n",
            "Progress:[   80/  120]  batch loss:0.151738\n",
            "Progress:[   88/  120]  batch loss:0.059046\n",
            "Progress:[   96/  120]  batch loss:0.004273\n",
            "Progress:[  104/  120]  batch loss:0.051513\n",
            "Progress:[  112/  120]  batch loss:0.011148\n",
            "Progress:[  120/  120]  batch loss:0.017600\n",
            "Train Error: Avg loss: 0.069558\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.073689\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004624\n",
            "Progress:[   16/  120]  batch loss:0.001266\n",
            "Progress:[   24/  120]  batch loss:0.098172\n",
            "Progress:[   32/  120]  batch loss:0.027797\n",
            "Progress:[   40/  120]  batch loss:0.001134\n",
            "Progress:[   48/  120]  batch loss:0.069045\n",
            "Progress:[   56/  120]  batch loss:0.028165\n",
            "Progress:[   64/  120]  batch loss:0.499217\n",
            "Progress:[   72/  120]  batch loss:0.006049\n",
            "Progress:[   80/  120]  batch loss:0.143810\n",
            "Progress:[   88/  120]  batch loss:0.063104\n",
            "Progress:[   96/  120]  batch loss:0.003822\n",
            "Progress:[  104/  120]  batch loss:0.055850\n",
            "Progress:[  112/  120]  batch loss:0.011752\n",
            "Progress:[  120/  120]  batch loss:0.018626\n",
            "Train Error: Avg loss: 0.068829\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.076786\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004813\n",
            "Progress:[   16/  120]  batch loss:0.001276\n",
            "Progress:[   24/  120]  batch loss:0.100188\n",
            "Progress:[   32/  120]  batch loss:0.027464\n",
            "Progress:[   40/  120]  batch loss:0.001105\n",
            "Progress:[   48/  120]  batch loss:0.070934\n",
            "Progress:[   56/  120]  batch loss:0.027886\n",
            "Progress:[   64/  120]  batch loss:0.501916\n",
            "Progress:[   72/  120]  batch loss:0.005692\n",
            "Progress:[   80/  120]  batch loss:0.152539\n",
            "Progress:[   88/  120]  batch loss:0.056480\n",
            "Progress:[   96/  120]  batch loss:0.003852\n",
            "Progress:[  104/  120]  batch loss:0.051684\n",
            "Progress:[  112/  120]  batch loss:0.011387\n",
            "Progress:[  120/  120]  batch loss:0.018079\n",
            "Train Error: Avg loss: 0.069020\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.080724\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004694\n",
            "Progress:[   16/  120]  batch loss:0.001296\n",
            "Progress:[   24/  120]  batch loss:0.103871\n",
            "Progress:[   32/  120]  batch loss:0.027872\n",
            "Progress:[   40/  120]  batch loss:0.001104\n",
            "Progress:[   48/  120]  batch loss:0.068097\n",
            "Progress:[   56/  120]  batch loss:0.030088\n",
            "Progress:[   64/  120]  batch loss:0.501608\n",
            "Progress:[   72/  120]  batch loss:0.006166\n",
            "Progress:[   80/  120]  batch loss:0.143726\n",
            "Progress:[   88/  120]  batch loss:0.058589\n",
            "Progress:[   96/  120]  batch loss:0.004031\n",
            "Progress:[  104/  120]  batch loss:0.057913\n",
            "Progress:[  112/  120]  batch loss:0.012409\n",
            "Progress:[  120/  120]  batch loss:0.019109\n",
            "Train Error: Avg loss: 0.069372\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.076074\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005115\n",
            "Progress:[   16/  120]  batch loss:0.001341\n",
            "Progress:[   24/  120]  batch loss:0.105780\n",
            "Progress:[   32/  120]  batch loss:0.029099\n",
            "Progress:[   40/  120]  batch loss:0.001094\n",
            "Progress:[   48/  120]  batch loss:0.069794\n",
            "Progress:[   56/  120]  batch loss:0.027217\n",
            "Progress:[   64/  120]  batch loss:0.498520\n",
            "Progress:[   72/  120]  batch loss:0.005866\n",
            "Progress:[   80/  120]  batch loss:0.147504\n",
            "Progress:[   88/  120]  batch loss:0.056252\n",
            "Progress:[   96/  120]  batch loss:0.003776\n",
            "Progress:[  104/  120]  batch loss:0.052320\n",
            "Progress:[  112/  120]  batch loss:0.010983\n",
            "Progress:[  120/  120]  batch loss:0.017234\n",
            "Train Error: Avg loss: 0.068793\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.076536\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004669\n",
            "Progress:[   16/  120]  batch loss:0.001260\n",
            "Progress:[   24/  120]  batch loss:0.101004\n",
            "Progress:[   32/  120]  batch loss:0.027094\n",
            "Progress:[   40/  120]  batch loss:0.001051\n",
            "Progress:[   48/  120]  batch loss:0.065890\n",
            "Progress:[   56/  120]  batch loss:0.030614\n",
            "Progress:[   64/  120]  batch loss:0.501512\n",
            "Progress:[   72/  120]  batch loss:0.006083\n",
            "Progress:[   80/  120]  batch loss:0.143986\n",
            "Progress:[   88/  120]  batch loss:0.056813\n",
            "Progress:[   96/  120]  batch loss:0.003784\n",
            "Progress:[  104/  120]  batch loss:0.057678\n",
            "Progress:[  112/  120]  batch loss:0.012568\n",
            "Progress:[  120/  120]  batch loss:0.019615\n",
            "Train Error: Avg loss: 0.068908\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.081073\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005170\n",
            "Progress:[   16/  120]  batch loss:0.001363\n",
            "Progress:[   24/  120]  batch loss:0.106317\n",
            "Progress:[   32/  120]  batch loss:0.029347\n",
            "Progress:[   40/  120]  batch loss:0.001066\n",
            "Progress:[   48/  120]  batch loss:0.073561\n",
            "Progress:[   56/  120]  batch loss:0.025749\n",
            "Progress:[   64/  120]  batch loss:0.494638\n",
            "Progress:[   72/  120]  batch loss:0.005808\n",
            "Progress:[   80/  120]  batch loss:0.150350\n",
            "Progress:[   88/  120]  batch loss:0.055404\n",
            "Progress:[   96/  120]  batch loss:0.004052\n",
            "Progress:[  104/  120]  batch loss:0.049147\n",
            "Progress:[  112/  120]  batch loss:0.010477\n",
            "Progress:[  120/  120]  batch loss:0.016324\n",
            "Train Error: Avg loss: 0.068585\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.070836\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004440\n",
            "Progress:[   16/  120]  batch loss:0.001232\n",
            "Progress:[   24/  120]  batch loss:0.095591\n",
            "Progress:[   32/  120]  batch loss:0.026582\n",
            "Progress:[   40/  120]  batch loss:0.001011\n",
            "Progress:[   48/  120]  batch loss:0.066398\n",
            "Progress:[   56/  120]  batch loss:0.026595\n",
            "Progress:[   64/  120]  batch loss:0.488290\n",
            "Progress:[   72/  120]  batch loss:0.006184\n",
            "Progress:[   80/  120]  batch loss:0.139149\n",
            "Progress:[   88/  120]  batch loss:0.062573\n",
            "Progress:[   96/  120]  batch loss:0.003196\n",
            "Progress:[  104/  120]  batch loss:0.055155\n",
            "Progress:[  112/  120]  batch loss:0.011280\n",
            "Progress:[  120/  120]  batch loss:0.017634\n",
            "Train Error: Avg loss: 0.067021\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.079144\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004684\n",
            "Progress:[   16/  120]  batch loss:0.001245\n",
            "Progress:[   24/  120]  batch loss:0.102417\n",
            "Progress:[   32/  120]  batch loss:0.025768\n",
            "Progress:[   40/  120]  batch loss:0.000962\n",
            "Progress:[   48/  120]  batch loss:0.064371\n",
            "Progress:[   56/  120]  batch loss:0.031130\n",
            "Progress:[   64/  120]  batch loss:0.506639\n",
            "Progress:[   72/  120]  batch loss:0.005788\n",
            "Progress:[   80/  120]  batch loss:0.149269\n",
            "Progress:[   88/  120]  batch loss:0.053413\n",
            "Progress:[   96/  120]  batch loss:0.003686\n",
            "Progress:[  104/  120]  batch loss:0.054857\n",
            "Progress:[  112/  120]  batch loss:0.012127\n",
            "Progress:[  120/  120]  batch loss:0.018637\n",
            "Train Error: Avg loss: 0.068999\n",
            "Test Error: Accuracy: 93.3%, Avg loss: 0.080717\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.005130\n",
            "Progress:[   16/  120]  batch loss:0.001372\n",
            "Progress:[   24/  120]  batch loss:0.108977\n",
            "Progress:[   32/  120]  batch loss:0.029558\n",
            "Progress:[   40/  120]  batch loss:0.001053\n",
            "Progress:[   48/  120]  batch loss:0.067857\n",
            "Progress:[   56/  120]  batch loss:0.028663\n",
            "Progress:[   64/  120]  batch loss:0.489025\n",
            "Progress:[   72/  120]  batch loss:0.006394\n",
            "Progress:[   80/  120]  batch loss:0.140741\n",
            "Progress:[   88/  120]  batch loss:0.054875\n",
            "Progress:[   96/  120]  batch loss:0.003961\n",
            "Progress:[  104/  120]  batch loss:0.055405\n",
            "Progress:[  112/  120]  batch loss:0.011775\n",
            "Progress:[  120/  120]  batch loss:0.017289\n",
            "Train Error: Avg loss: 0.068138\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.069545\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004934\n",
            "Progress:[   16/  120]  batch loss:0.001296\n",
            "Progress:[   24/  120]  batch loss:0.102105\n",
            "Progress:[   32/  120]  batch loss:0.027833\n",
            "Progress:[   40/  120]  batch loss:0.000993\n",
            "Progress:[   48/  120]  batch loss:0.065279\n",
            "Progress:[   56/  120]  batch loss:0.027798\n",
            "Progress:[   64/  120]  batch loss:0.490836\n",
            "Progress:[   72/  120]  batch loss:0.006114\n",
            "Progress:[   80/  120]  batch loss:0.141628\n",
            "Progress:[   88/  120]  batch loss:0.055341\n",
            "Progress:[   96/  120]  batch loss:0.003433\n",
            "Progress:[  104/  120]  batch loss:0.054157\n",
            "Progress:[  112/  120]  batch loss:0.011175\n",
            "Progress:[  120/  120]  batch loss:0.017044\n",
            "Train Error: Avg loss: 0.067331\n",
            "Test Error: Accuracy: 96.7%, Avg loss: 0.075400\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004828\n",
            "Progress:[   16/  120]  batch loss:0.001265\n",
            "Progress:[   24/  120]  batch loss:0.097339\n",
            "Progress:[   32/  120]  batch loss:0.027415\n",
            "Progress:[   40/  120]  batch loss:0.000935\n",
            "Progress:[   48/  120]  batch loss:0.076492\n",
            "Progress:[   56/  120]  batch loss:0.022902\n",
            "Progress:[   64/  120]  batch loss:0.487744\n",
            "Progress:[   72/  120]  batch loss:0.005643\n",
            "Progress:[   80/  120]  batch loss:0.152734\n",
            "Progress:[   88/  120]  batch loss:0.049712\n",
            "Progress:[   96/  120]  batch loss:0.003876\n",
            "Progress:[  104/  120]  batch loss:0.045746\n",
            "Progress:[  112/  120]  batch loss:0.009998\n",
            "Progress:[  120/  120]  batch loss:0.015017\n",
            "Train Error: Avg loss: 0.066776\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.066096\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004262\n",
            "Progress:[   16/  120]  batch loss:0.001217\n",
            "Progress:[   24/  120]  batch loss:0.092052\n",
            "Progress:[   32/  120]  batch loss:0.025918\n",
            "Progress:[   40/  120]  batch loss:0.000947\n",
            "Progress:[   48/  120]  batch loss:0.070282\n",
            "Progress:[   56/  120]  batch loss:0.024325\n",
            "Progress:[   64/  120]  batch loss:0.473988\n",
            "Progress:[   72/  120]  batch loss:0.006405\n",
            "Progress:[   80/  120]  batch loss:0.138837\n",
            "Progress:[   88/  120]  batch loss:0.057974\n",
            "Progress:[   96/  120]  batch loss:0.003252\n",
            "Progress:[  104/  120]  batch loss:0.052848\n",
            "Progress:[  112/  120]  batch loss:0.011169\n",
            "Progress:[  120/  120]  batch loss:0.016130\n",
            "Train Error: Avg loss: 0.065307\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.069164\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004579\n",
            "Progress:[   16/  120]  batch loss:0.001235\n",
            "Progress:[   24/  120]  batch loss:0.094796\n",
            "Progress:[   32/  120]  batch loss:0.025770\n",
            "Progress:[   40/  120]  batch loss:0.000901\n",
            "Progress:[   48/  120]  batch loss:0.064430\n",
            "Progress:[   56/  120]  batch loss:0.026791\n",
            "Progress:[   64/  120]  batch loss:0.488939\n",
            "Progress:[   72/  120]  batch loss:0.006139\n",
            "Progress:[   80/  120]  batch loss:0.141054\n",
            "Progress:[   88/  120]  batch loss:0.054143\n",
            "Progress:[   96/  120]  batch loss:0.003260\n",
            "Progress:[  104/  120]  batch loss:0.053748\n",
            "Progress:[  112/  120]  batch loss:0.011183\n",
            "Progress:[  120/  120]  batch loss:0.016727\n",
            "Train Error: Avg loss: 0.066246\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.069027\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004805\n",
            "Progress:[   16/  120]  batch loss:0.001252\n",
            "Progress:[   24/  120]  batch loss:0.098452\n",
            "Progress:[   32/  120]  batch loss:0.026907\n",
            "Progress:[   40/  120]  batch loss:0.000920\n",
            "Progress:[   48/  120]  batch loss:0.067117\n",
            "Progress:[   56/  120]  batch loss:0.026124\n",
            "Progress:[   64/  120]  batch loss:0.481694\n",
            "Progress:[   72/  120]  batch loss:0.006203\n",
            "Progress:[   80/  120]  batch loss:0.142973\n",
            "Progress:[   88/  120]  batch loss:0.050971\n",
            "Progress:[   96/  120]  batch loss:0.003290\n",
            "Progress:[  104/  120]  batch loss:0.051545\n",
            "Progress:[  112/  120]  batch loss:0.011102\n",
            "Progress:[  120/  120]  batch loss:0.016344\n",
            "Train Error: Avg loss: 0.065980\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.069217\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004671\n",
            "Progress:[   16/  120]  batch loss:0.001266\n",
            "Progress:[   24/  120]  batch loss:0.097155\n",
            "Progress:[   32/  120]  batch loss:0.026769\n",
            "Progress:[   40/  120]  batch loss:0.000907\n",
            "Progress:[   48/  120]  batch loss:0.072533\n",
            "Progress:[   56/  120]  batch loss:0.023375\n",
            "Progress:[   64/  120]  batch loss:0.477074\n",
            "Progress:[   72/  120]  batch loss:0.006159\n",
            "Progress:[   80/  120]  batch loss:0.143855\n",
            "Progress:[   88/  120]  batch loss:0.049508\n",
            "Progress:[   96/  120]  batch loss:0.003363\n",
            "Progress:[  104/  120]  batch loss:0.049496\n",
            "Progress:[  112/  120]  batch loss:0.010385\n",
            "Progress:[  120/  120]  batch loss:0.014965\n",
            "Train Error: Avg loss: 0.065432\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.064894\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004504\n",
            "Progress:[   16/  120]  batch loss:0.001209\n",
            "Progress:[   24/  120]  batch loss:0.092172\n",
            "Progress:[   32/  120]  batch loss:0.026044\n",
            "Progress:[   40/  120]  batch loss:0.000858\n",
            "Progress:[   48/  120]  batch loss:0.068208\n",
            "Progress:[   56/  120]  batch loss:0.023811\n",
            "Progress:[   64/  120]  batch loss:0.479335\n",
            "Progress:[   72/  120]  batch loss:0.006300\n",
            "Progress:[   80/  120]  batch loss:0.139307\n",
            "Progress:[   88/  120]  batch loss:0.053659\n",
            "Progress:[   96/  120]  batch loss:0.003172\n",
            "Progress:[  104/  120]  batch loss:0.051788\n",
            "Progress:[  112/  120]  batch loss:0.010578\n",
            "Progress:[  120/  120]  batch loss:0.015595\n",
            "Train Error: Avg loss: 0.065103\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.065259\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004580\n",
            "Progress:[   16/  120]  batch loss:0.001212\n",
            "Progress:[   24/  120]  batch loss:0.092240\n",
            "Progress:[   32/  120]  batch loss:0.025477\n",
            "Progress:[   40/  120]  batch loss:0.000866\n",
            "Progress:[   48/  120]  batch loss:0.062373\n",
            "Progress:[   56/  120]  batch loss:0.027568\n",
            "Progress:[   64/  120]  batch loss:0.480571\n",
            "Progress:[   72/  120]  batch loss:0.006488\n",
            "Progress:[   80/  120]  batch loss:0.136945\n",
            "Progress:[   88/  120]  batch loss:0.053369\n",
            "Progress:[   96/  120]  batch loss:0.003021\n",
            "Progress:[  104/  120]  batch loss:0.055159\n",
            "Progress:[  112/  120]  batch loss:0.011751\n",
            "Progress:[  120/  120]  batch loss:0.016789\n",
            "Train Error: Avg loss: 0.065227\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.069562\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Progress:[    8/  120]  batch loss:0.004856\n",
            "Progress:[   16/  120]  batch loss:0.001289\n",
            "Progress:[   24/  120]  batch loss:0.100144\n",
            "Progress:[   32/  120]  batch loss:0.026349\n",
            "Progress:[   40/  120]  batch loss:0.000890\n",
            "Progress:[   48/  120]  batch loss:0.067142\n",
            "Progress:[   56/  120]  batch loss:0.026900\n",
            "Progress:[   64/  120]  batch loss:0.477439\n",
            "Progress:[   72/  120]  batch loss:0.006326\n",
            "Progress:[   80/  120]  batch loss:0.143297\n",
            "Progress:[   88/  120]  batch loss:0.047498\n",
            "Progress:[   96/  120]  batch loss:0.003358\n",
            "Progress:[  104/  120]  batch loss:0.051309\n",
            "Progress:[  112/  120]  batch loss:0.011082\n",
            "Progress:[  120/  120]  batch loss:0.015796\n",
            "Train Error: Avg loss: 0.065578\n",
            "Test Error: Accuracy: 100.0%, Avg loss: 0.064232\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and test loss\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs'), plt.ylabel('Loss'), plt.title('Training and Test Loss Over Epochs')\n",
        "plt.legend(), plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "zL0y7Y1IB0CS",
        "outputId": "9a2c0444-782d-4549-f405-c6c707d9e2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<matplotlib.legend.Legend at 0x7ffa3eb7d2d0>, None)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}