{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "245fb301-fb2c-4be0-90c5-ba5dc81f63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb994dea-c78b-4c1b-8a37-b1a045bdb14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mohsinshah_umass_edu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16902c97-b1f0-446a-8468-f425d33a8e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef21db214b1441af90188f541c3c24ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/18.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fba3fea64294fdea8f8ce5580cfa742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/6.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d41c89dbb24f19a3947b4ea3296e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d658b1cbbc4d2c9d8c46bb66578475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1565e916cf8481496ac233c93b9e3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/576M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a53d6637114b5d90f442ff8f50ec70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/21.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d2ea34483943eaa317eaeb9182a421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0c1bfa25ea4f20a798fca664e0e937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b490b792f44e76b3d3f735dbe22bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3db99fe15904b65ae0647b63dbba6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4a2c0c21bf4552bb9c1125f18c1547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498c5264cdf24444846ddc0da88014bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/18.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bd939f72ca42f485b78e80f2cfc186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/36.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb75990-37e1-4a86-9be1-c918d071f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b8cfd7-01cb-4fba-9b85-cd2be1590637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'jhwod',\n",
       " 'title': 'A mathy genotype-phenotype puzzle.',\n",
       " 'selftext': 'This will likely turn out to be a math question, but nonetheless, this subreddit seems the most appropriate place to ask it.\\n\\nBiologists can talk about the \"space of all possible genomes\" (of a certain length, perhaps) created from strings of the typical alphabet {A,C,G,T}. Dennett has called this the \"library of Mendel\", which is who I was reading when this question occurred to me.\\n\\nEach living organism possesses a genome from within that space, but (because of the [genotype/phenotype distinction](_URL_0_)) a genome alone does not describe an organism. Focusing on some arbitrary sexually reproducing organism, you could say that the information in the genome is \"read\" by the phenotype. To illustrate what I mean: a few nucleotides on a table won\\'t become a bird, but those same nucleotides housed carefully in an egg will. And later on, that bird will take to laying similar eggs.\\n\\nIn the book I\\'m reading (Darwin\\'s Dangerous Idea), Dennett suggests a Good Way of thinking about this relationship: the reproductive system (or perhaps the whole phenotype) is a machine for copying the genome, which in turn provides instructions to another new phenotype.\\n\\nDennett seems to assume that some (probably most) genomes can\\'t possibly have a corresponding phenotype that would reproduce them -- that the ones that do exist are special cases. My intuition is the opposite: information processing (and copying) is information processing. Sure, some genomes might require very complicated, unprecedented phenotypes in order to be copied, but in principle, it\\'s all the same thing, isn\\'t it?\\n\\nFinally, the question: Is there any reason to think that there exist genomes for which a physically realizable copying machine (phenotype) in principle can\\'t be built? Any reason to think once can\\'t in principle evolve?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['c2ccomy', 'c2ccldo'],\n",
       "  'text': [\"I like this question.  \\n\\nOne thing to consider is that the machinery required for protein synthesis is also tooled via... protein synthesis.  In order to stamp out a protein, you need a ribosome, which is a big complex of ribosomal proteins and rRNA, all of which have to get transcribed in the first place.  So a genome that doesn't code for that machinery is going to have a difficult time propagating itself, assuming that it can't access that machinery in some other way (see below).\\n\\nAnother counterexample: a single letter genome.  The lower limit on a genome is astonishingly small (~2kbp in some [viruses](_URL_0_)) but it's still positive.  As the size of the genome decreases, the phenotype has to make up the slack.  At some point, the phenotype would have to start carrying its own instructions for replication, and you're no longer talking about _the_ genome, but rather _a part of it_.  In fact, this is effectively what you see with viruses: you can pare down the genome because a significant chunk of the instructions required to replicate a virus is _stored in another organism_.\",\n",
       "   'A hypothetical alien organism that still happened to be DNA-based could in principle have any genome sequence. Here on Earth, however, the basic rules of the genetic code are pretty much \"locked in\", and these restrictions prevent species from colonizing most of \"genome space.\" For example, it would be practically impossible for a terrestrial species to evolve in which amino acids were specified by a string of five bases, instead of three, because that would require millions of parallel mutations simultaneously. Think of the analogy with language: not all combinations of letters (genotype) can make a coherent story (phenotype) in English (the Earth genetic code). However, there are hypothetical languages that could make sense of any combination of letters.'],\n",
       "  'score': [3, 2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': ['http://en.wikipedia.org/wiki/Genotype-phenotype_distinction']},\n",
       " 'answers_urls': {'url': ['http://en.wikipedia.org/wiki/Circoviridae']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c628209-877b-4ce5-9765-cb6aee81c235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e4777ac88a4cc2a61bff890cd670f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6918e6025e44209624d71c48077443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bd185b0f4f4b97be3587fd6ba7402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77684e7567f4e2a9eb8ae7eb68400cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506907cc-0543-4ec8-ba97-cb2fdd614d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb6a996-0492-40f5-8a29-0a16635f1c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'jhwod',\n",
       " 'title': 'A mathy genotype-phenotype puzzle.',\n",
       " 'selftext': 'This will likely turn out to be a math question, but nonetheless, this subreddit seems the most appropriate place to ask it.\\n\\nBiologists can talk about the \"space of all possible genomes\" (of a certain length, perhaps) created from strings of the typical alphabet {A,C,G,T}. Dennett has called this the \"library of Mendel\", which is who I was reading when this question occurred to me.\\n\\nEach living organism possesses a genome from within that space, but (because of the [genotype/phenotype distinction](_URL_0_)) a genome alone does not describe an organism. Focusing on some arbitrary sexually reproducing organism, you could say that the information in the genome is \"read\" by the phenotype. To illustrate what I mean: a few nucleotides on a table won\\'t become a bird, but those same nucleotides housed carefully in an egg will. And later on, that bird will take to laying similar eggs.\\n\\nIn the book I\\'m reading (Darwin\\'s Dangerous Idea), Dennett suggests a Good Way of thinking about this relationship: the reproductive system (or perhaps the whole phenotype) is a machine for copying the genome, which in turn provides instructions to another new phenotype.\\n\\nDennett seems to assume that some (probably most) genomes can\\'t possibly have a corresponding phenotype that would reproduce them -- that the ones that do exist are special cases. My intuition is the opposite: information processing (and copying) is information processing. Sure, some genomes might require very complicated, unprecedented phenotypes in order to be copied, but in principle, it\\'s all the same thing, isn\\'t it?\\n\\nFinally, the question: Is there any reason to think that there exist genomes for which a physically realizable copying machine (phenotype) in principle can\\'t be built? Any reason to think once can\\'t in principle evolve?',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c2ccomy', 'c2ccldo'],\n",
       " 'answers.text': [\"I like this question.  \\n\\nOne thing to consider is that the machinery required for protein synthesis is also tooled via... protein synthesis.  In order to stamp out a protein, you need a ribosome, which is a big complex of ribosomal proteins and rRNA, all of which have to get transcribed in the first place.  So a genome that doesn't code for that machinery is going to have a difficult time propagating itself, assuming that it can't access that machinery in some other way (see below).\\n\\nAnother counterexample: a single letter genome.  The lower limit on a genome is astonishingly small (~2kbp in some [viruses](_URL_0_)) but it's still positive.  As the size of the genome decreases, the phenotype has to make up the slack.  At some point, the phenotype would have to start carrying its own instructions for replication, and you're no longer talking about _the_ genome, but rather _a part of it_.  In fact, this is effectively what you see with viruses: you can pare down the genome because a significant chunk of the instructions required to replicate a virus is _stored in another organism_.\",\n",
       "  'A hypothetical alien organism that still happened to be DNA-based could in principle have any genome sequence. Here on Earth, however, the basic rules of the genetic code are pretty much \"locked in\", and these restrictions prevent species from colonizing most of \"genome space.\" For example, it would be practically impossible for a terrestrial species to evolve in which amino acids were specified by a string of five bases, instead of three, because that would require millions of parallel mutations simultaneously. Think of the analogy with language: not all combinations of letters (genotype) can make a coherent story (phenotype) in English (the Earth genetic code). However, there are hypothetical languages that could make sense of any combination of letters.'],\n",
       " 'answers.score': [3, 2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': ['http://en.wikipedia.org/wiki/Genotype-phenotype_distinction'],\n",
       " 'answers_urls.url': ['http://en.wikipedia.org/wiki/Circoviridae']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727a4369-e1c1-430f-9711-b0ba2444e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a107203-7b3e-413e-9046-02c21eedca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da5301db67445f190c958b633ef109e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1291 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1462 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14381 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846aac7b6cd549f9b2f21cd8a47f765b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1774 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3333 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5931 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7fbd9d-8280-4f92-b6e1-de6c2a311986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[40, 588, 428, 1808, 13, 220, 220, 198, 198, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [40, 588, 428, 1808, 13, 220, 220, 198, 198, 3...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokenized_eli5[\"train\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51adaac-5393-40fd-b05b-14a28d5a5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a4731c3-f593-4531-8b37-fffd98ffac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d2280bca304161961a86c1a0c8b30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84823f9870c42adae0eb2a70b4d1cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f75745e8-daf2-4964-be95-ae05fe8bb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdbd18c9-f08b-4cb4-89b1-79935cc87437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f8720408d84d3c89d875c3c3de8e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c8d18fbfe842599d89175b6f89fa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bcd1e59-563b-4b38-971e-c742e72b0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0a9462b-f37e-4a75-b5fb-4b60365c40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/mohsinshah/test_eli5_clm-model into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb314907-2f18-4d7b-b0f4-c7cab39890d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsinshah_umass_edu/.conda/envs/pytorchGPU/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3393' max='3393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3393/3393 07:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.864200</td>\n",
       "      <td>3.775990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.763700</td>\n",
       "      <td>3.759658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.735600</td>\n",
       "      <td>3.756453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3393, training_loss=3.796242666427203, metrics={'train_runtime': 430.3711, 'train_samples_per_second': 63.036, 'train_steps_per_second': 7.884, 'total_flos': 886089943351296.0, 'train_loss': 3.796242666427203, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f812ed99-42f0-47dc-99ae-af352e208841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0a62671a1a4b12b4cc6e7853841fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecc6dcaa4704791990902f1a2ae360c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Dec31_18-03-40_gypsum-gpu186/events.out.tfevents.1704045827.gypsum-gpu186:   0%|          | 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/mohsinshah/test_eli5_clm-model\n",
      "   91e7848..642e3bd  main -> main\n",
      "\n",
      "To https://huggingface.co/mohsinshah/test_eli5_clm-model\n",
      "   642e3bd..b1baa0c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mohsinshah/test_eli5_clm-model/commit/642e3bdef2445a3f3c1e367fa3be8cdc4a0fead3'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5afbe4ec-d2d2-433d-854e-1c50be8ca3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 42.80\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "pytorchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
